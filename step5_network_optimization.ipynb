{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Step 5: Network Optimization\n",
    "\n",
    "**Iteratively optimize 20 networks to match reference distributions**\n",
    "\n",
    "This notebook:\n",
    "1. Loads 20 initial networks from Step 4 (with syntax metrics)\n",
    "2. Defines optimization objectives based on reference city distributions\n",
    "3. Iteratively optimizes each network:\n",
    "   - Match segment length distribution\n",
    "   - Match orientation distribution\n",
    "   - Match degree distribution\n",
    "   - Improve intelligibility\n",
    "   - Adjust node density\n",
    "4. Uses simulated annealing / genetic algorithm for optimization\n",
    "5. Saves optimized networks for ranking and selection\n",
    "\n",
    "**Optimization strategies:**\n",
    "- Add/remove edges to match degree distribution\n",
    "- Perturb node positions to match segment lengths\n",
    "- Remove/add edges to match orientation histogram\n",
    "- Multiple iterations with cooling schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from scipy.spatial import Delaunay\n",
    "import copy\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE_M = 500  # 500×500m window\n",
    "\n",
    "# Optimization parameters\n",
    "NUM_ITERATIONS = 100  # Number of optimization iterations per network\n",
    "INITIAL_TEMP = 1.0    # Initial temperature for simulated annealing\n",
    "COOLING_RATE = 0.95   # Temperature cooling rate\n",
    "MIN_TEMP = 0.01       # Minimum temperature\n",
    "\n",
    "# Create output directories\n",
    "Path(\"outputs/generated/visualizations\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"outputs/generated/optimized\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Window size: {WINDOW_SIZE_M}m × {WINDOW_SIZE_M}m\")\n",
    "print(f\"Optimization iterations: {NUM_ITERATIONS}\")\n",
    "print(f\"Temperature: {INITIAL_TEMP} → {MIN_TEMP} (cooling: {COOLING_RATE})\")\n",
    "print(\"✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference city data\n",
    "with open('outputs/data/reference_cities_data.pkl', 'rb') as f:\n",
    "    reference_data = pickle.load(f)\n",
    "\n",
    "# Load generated networks with space syntax from Step 4\n",
    "with open('outputs/generated/syntax/networks_with_syntax_20.pkl', 'rb') as f:\n",
    "    generated_networks = pickle.load(f)\n",
    "\n",
    "print(\"✓ Loaded reference data from Step 1\")\n",
    "print(f\"✓ Loaded {len(generated_networks)} networks from Step 4\")\n",
    "\n",
    "# Reference city for optimization (London)\n",
    "reference_city = 'london'\n",
    "ref_data = reference_data[reference_city]\n",
    "\n",
    "print(f\"\\nUsing {reference_city.upper()} as optimization target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Extract Reference Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target distributions from reference city\n",
    "ref_segment_lengths = ref_data['morphology']['segment_lengths']\n",
    "ref_degree_dist = ref_data['morphology']['degree_distribution']\n",
    "ref_orientation_hist = ref_data['morphology']['orientation_hist']\n",
    "\n",
    "# Target metrics (adjusted for doubled roads where needed)\n",
    "target_metrics = {\n",
    "    'node_density': ref_data['morphology']['node_density'] / 2,  # HALVED\n",
    "    'avg_degree': ref_data['morphology']['avg_degree'] / 2,      # HALVED\n",
    "    'avg_segment_length': ref_data['morphology']['avg_segment_length'],  # NOT doubled\n",
    "    'intelligibility': ref_data['syntax']['intelligibility'],\n",
    "    'mean_depth': ref_data['syntax']['mean_depth']\n",
    "}\n",
    "\n",
    "# Compute segment length probability distribution\n",
    "seg_counts, seg_bins = np.histogram(ref_segment_lengths, bins=30, range=(5, 150))\n",
    "seg_probs = seg_counts / seg_counts.sum()\n",
    "\n",
    "# Orientation probability distribution\n",
    "ori_bins, ori_counts = ref_orientation_hist\n",
    "ori_probs = ori_counts / ori_counts.sum() if ori_counts.sum() > 0 else ori_counts\n",
    "\n",
    "print(\"Target Metrics:\")\n",
    "print(\"=\"*50)\n",
    "for k, v in target_metrics.items():\n",
    "    print(f\"  {k:25s}: {v:.3f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nSegment length distribution: {len(seg_bins)-1} bins\")\n",
    "print(f\"Orientation distribution: {len(ori_bins)-1} bins\")\n",
    "print(f\"Degree distribution: {len(ref_degree_dist)} unique degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "def compute_fitness(G, pos, target_metrics, seg_probs, seg_bins, ori_probs, ori_bins):\n    \"\"\"\n    Compute fitness score for a network.\n    Lower score = better fit to target distributions.\n    \n    Components:\n    1. Segment length distribution similarity (KL divergence)\n    2. Orientation distribution similarity (KL divergence)\n    3. Metric differences (node density, avg degree, intelligibility)\n    \"\"\"\n    fitness = 0.0\n    \n    # 1. Segment length distribution\n    lengths = [d['length'] for u, v, d in G.edges(data=True) if 'length' in d]\n    if lengths:\n        counts, _ = np.histogram(lengths, bins=seg_bins)\n        probs = counts / counts.sum() if counts.sum() > 0 else counts\n        probs = np.clip(probs, 1e-10, 1)  # Avoid log(0)\n        seg_probs_clipped = np.clip(seg_probs, 1e-10, 1)\n        kl_seg = np.sum(probs * np.log(probs / seg_probs_clipped))\n        fitness += kl_seg * 2.0  # Weight\n    \n    # 2. Orientation distribution\n    bearings = []\n    for u, v in G.edges():\n        dx = pos[v][0] - pos[u][0]\n        dy = pos[v][1] - pos[u][1]\n        angle = math.atan2(dy, dx)\n        bearing = math.degrees(angle) % 180\n        bearings.append(bearing)\n    \n    if bearings:\n        counts, _ = np.histogram(bearings, bins=ori_bins)\n        probs = counts / counts.sum() if counts.sum() > 0 else counts\n        probs = np.clip(probs, 1e-10, 1)\n        ori_probs_clipped = np.clip(ori_probs, 1e-10, 1)\n        kl_ori = np.sum(probs * np.log(probs / ori_probs_clipped))\n        fitness += kl_ori * 1.5  # Weight\n    \n    # 3. Metric differences\n    area_km2 = (WINDOW_SIZE_M / 1000.0) ** 2\n    node_density = G.number_of_nodes() / area_km2\n    degrees = [d for _, d in G.degree()]\n    avg_degree = np.mean(degrees) if degrees else 0\n    avg_seg_len = np.mean(lengths) if lengths else 0\n    \n    # Normalized differences\n    fitness += abs(node_density - target_metrics['node_density']) / target_metrics['node_density'] * 1.0\n    fitness += abs(avg_degree - target_metrics['avg_degree']) / target_metrics['avg_degree'] * 1.0\n    fitness += abs(avg_seg_len - target_metrics['avg_segment_length']) / target_metrics['avg_segment_length'] * 1.5\n    \n    return fitness\n\n\ndef perturb_network(G, pos, temperature):\n    \"\"\"\n    Apply random perturbation to network.\n    \n    Operations:\n    1. Move random node (small displacement)\n    2. Add random edge (with probability)\n    3. Remove random edge (with probability)\n    \"\"\"\n    G_new = G.copy()\n    pos_new = pos.copy()\n    \n    # Choose operation based on temperature\n    operation = np.random.choice(['move_node', 'add_edge', 'remove_edge'], p=[0.5, 0.25, 0.25])\n    \n    if operation == 'move_node' and len(pos_new) > 0:\n        # Move random node\n        node = np.random.choice(list(pos_new.keys()))\n        max_displacement = temperature * 20  # Scale with temperature\n        dx = np.random.uniform(-max_displacement, max_displacement)\n        dy = np.random.uniform(-max_displacement, max_displacement)\n        \n        new_x = np.clip(pos_new[node][0] + dx, 10, WINDOW_SIZE_M - 10)\n        new_y = np.clip(pos_new[node][1] + dy, 10, WINDOW_SIZE_M - 10)\n        pos_new[node] = (new_x, new_y)\n        \n        # Update edge lengths for ALL edges connected to this node\n        # For undirected graphs, neighbors() gives all connected nodes\n        for neighbor in list(G_new.neighbors(node)):\n            length = np.sqrt((pos_new[node][0] - pos_new[neighbor][0])**2 + \n                           (pos_new[node][1] - pos_new[neighbor][1])**2)\n            if G_new.has_edge(node, neighbor):\n                G_new[node][neighbor]['length'] = length\n    \n    elif operation == 'add_edge' and len(pos_new) > 1:\n        # Add random edge between nearby nodes\n        nodes = list(G_new.nodes())\n        u = np.random.choice(nodes)\n        \n        # Find nearby nodes\n        candidates = []\n        for v in nodes:\n            if v != u and not G_new.has_edge(u, v):\n                dist = np.sqrt((pos_new[u][0] - pos_new[v][0])**2 + \n                             (pos_new[u][1] - pos_new[v][1])**2)\n                if dist < 100:  # Max connection distance\n                    candidates.append((v, dist))\n        \n        if candidates:\n            v, length = candidates[np.random.randint(len(candidates))]\n            G_new.add_edge(u, v, length=length)\n    \n    elif operation == 'remove_edge' and G_new.number_of_edges() > 0:\n        # Remove random edge (but don't disconnect graph)\n        edges = list(G_new.edges())\n        if edges:\n            u, v = edges[np.random.randint(len(edges))]\n            G_new.remove_edge(u, v)\n            \n            # Check if still connected\n            if not nx.is_connected(G_new):\n                # Restore edge if it disconnects graph\n                length = np.sqrt((pos_new[u][0] - pos_new[v][0])**2 + \n                               (pos_new[u][1] - pos_new[v][1])**2)\n                G_new.add_edge(u, v, length=length)\n    \n    return G_new, pos_new\n\n\ndef optimize_network(G_init, pos_init, target_metrics, seg_probs, seg_bins, ori_probs, ori_bins,\n                    num_iterations=100, initial_temp=1.0, cooling_rate=0.95):\n    \"\"\"\n    Optimize network using simulated annealing.\n    \n    HOW IT WORKS:\n    1. Start with initial network and compute fitness (how far from target)\n    2. Each iteration:\n       - Randomly perturb network (move node, add/remove edge)\n       - Compute new fitness\n       - Accept if better, or accept worse with probability exp(-delta/T)\n       - Cool down temperature\n    3. Return best network found\n    \n    Fitness measures:\n    - KL divergence between segment length distributions\n    - KL divergence between orientation distributions  \n    - Difference in node density, avg degree, avg segment length\n    \n    Returns:\n        Optimized graph, positions, fitness history\n    \"\"\"\n    G_current = G_init.copy()\n    pos_current = pos_init.copy()\n    fitness_current = compute_fitness(G_current, pos_current, target_metrics, \n                                     seg_probs, seg_bins, ori_probs, ori_bins)\n    \n    G_best = G_current.copy()\n    pos_best = pos_current.copy()\n    fitness_best = fitness_current\n    \n    fitness_history = [fitness_current]\n    temperature = initial_temp\n    \n    for iteration in range(num_iterations):\n        # Perturb network\n        G_new, pos_new = perturb_network(G_current, pos_current, temperature)\n        \n        # Compute new fitness\n        fitness_new = compute_fitness(G_new, pos_new, target_metrics,\n                                     seg_probs, seg_bins, ori_probs, ori_bins)\n        \n        # Accept or reject (simulated annealing)\n        delta = fitness_new - fitness_current\n        \n        if delta < 0 or np.random.random() < np.exp(-delta / temperature):\n            # Accept\n            G_current = G_new\n            pos_current = pos_new\n            fitness_current = fitness_new\n            \n            # Update best\n            if fitness_current < fitness_best:\n                G_best = G_current.copy()\n                pos_best = pos_current.copy()\n                fitness_best = fitness_current\n        \n        fitness_history.append(fitness_current)\n        \n        # Cool down\n        temperature = max(temperature * cooling_rate, MIN_TEMP)\n        \n        # Progress\n        if (iteration + 1) % 20 == 0:\n            print(f\"    Iter {iteration+1:3d}: fitness={fitness_current:.4f}, best={fitness_best:.4f}, T={temperature:.3f}\")\n    \n    return G_best, pos_best, fitness_history\n\n\nprint(\"✓ Optimization functions defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Optimize All 20 Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimizing 20 networks...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "optimized_networks = []\n",
    "\n",
    "for idx, network_data in enumerate(generated_networks):\n",
    "    net_id = network_data['id']\n",
    "    G_init = network_data['graph']\n",
    "    pos_init = network_data['pos']\n",
    "    \n",
    "    print(f\"\\nNetwork {net_id+1:2d}/{len(generated_networks)}:\")\n",
    "    print(f\"  Initial: {G_init.number_of_nodes()} nodes, {G_init.number_of_edges()} edges\")\n",
    "    \n",
    "    # Compute initial fitness\n",
    "    fitness_init = compute_fitness(G_init, pos_init, target_metrics,\n",
    "                                   seg_probs, seg_bins, ori_probs, ori_bins)\n",
    "    print(f\"  Initial fitness: {fitness_init:.4f}\")\n",
    "    \n",
    "    # Optimize\n",
    "    G_opt, pos_opt, fitness_history = optimize_network(\n",
    "        G_init, pos_init, target_metrics, seg_probs, seg_bins, ori_probs, ori_bins,\n",
    "        num_iterations=NUM_ITERATIONS,\n",
    "        initial_temp=INITIAL_TEMP,\n",
    "        cooling_rate=COOLING_RATE\n",
    "    )\n",
    "    \n",
    "    print(f\"  Final: {G_opt.number_of_nodes()} nodes, {G_opt.number_of_edges()} edges\")\n",
    "    print(f\"  Final fitness: {fitness_history[-1]:.4f}\")\n",
    "    print(f\"  Improvement: {(fitness_init - fitness_history[-1]) / fitness_init * 100:.1f}%\")\n",
    "    \n",
    "    # Store optimized network\n",
    "    optimized_data = {\n",
    "        'id': net_id,\n",
    "        'graph': G_opt,\n",
    "        'pos': pos_opt,\n",
    "        'fitness_history': fitness_history,\n",
    "        'initial_fitness': fitness_init,\n",
    "        'final_fitness': fitness_history[-1],\n",
    "        'improvement': (fitness_init - fitness_history[-1]) / fitness_init\n",
    "    }\n",
    "    optimized_networks.append(optimized_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ Optimization complete for all 20 networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Optimization Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*20 + \"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Network':<10} {'Initial':<12} {'Final':<12} {'Improvement':<15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for net in optimized_networks:\n",
    "    net_id = net['id'] + 1\n",
    "    init_fit = net['initial_fitness']\n",
    "    final_fit = net['final_fitness']\n",
    "    improvement = net['improvement'] * 100\n",
    "    \n",
    "    print(f\"{net_id:<10} {init_fit:<12.4f} {final_fit:<12.4f} {improvement:<15.1f}%\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_improvement = np.mean([net['improvement'] for net in optimized_networks]) * 100\n",
    "print(f\"\\nAverage improvement: {avg_improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Visualize Optimization Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fitness histories for all networks\n",
    "fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, net in enumerate(optimized_networks):\n",
    "    ax = axes[idx]\n",
    "    history = net['fitness_history']\n",
    "    \n",
    "    ax.plot(history, linewidth=2, color='steelblue', alpha=0.7)\n",
    "    ax.axhline(net['initial_fitness'], color='red', linestyle='--', \n",
    "              linewidth=1, alpha=0.5, label='Initial')\n",
    "    ax.axhline(net['final_fitness'], color='green', linestyle='--', \n",
    "              linewidth=1, alpha=0.5, label='Final')\n",
    "    \n",
    "    ax.set_xlabel('Iteration', fontsize=9)\n",
    "    ax.set_ylabel('Fitness', fontsize=9)\n",
    "    ax.set_title(f'Network {net[\"id\"]+1}\\nImprovement: {net[\"improvement\"]*100:.1f}%',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Optimization Progress: All 20 Networks', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('outputs/generated/visualizations/E1_optimization_progress.svg',\n",
    "           format='svg', bbox_inches='tight', dpi=300)\n",
    "print(\"Saved: outputs/generated/visualizations/E1_optimization_progress.svg\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Save Optimized Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized networks\n",
    "with open('outputs/generated/optimized/optimized_networks_20.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized_networks, f)\n",
    "\n",
    "print(\"✓ Saved optimized networks to: outputs/generated/optimized/optimized_networks_20.pkl\")\n",
    "\n",
    "print(\"\\nEach optimized network includes:\")\n",
    "print(\"  - Optimized NetworkX graph\")\n",
    "print(\"  - Optimized node positions\")\n",
    "print(\"  - Fitness history (optimization trajectory)\")\n",
    "print(\"  - Initial and final fitness scores\")\n",
    "print(\"  - Improvement percentage\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✓ STEP 5 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOptimized 20 networks to match {reference_city.upper()} distributions\")\n",
    "print(f\"Average improvement: {avg_improvement:.1f}%\")\n",
    "print(\"\\nNext: Rank and select best network (Step 6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Step 5 Complete!**\n",
    "\n",
    "Optimization Results:\n",
    "- Applied simulated annealing to all 20 networks\n",
    "- Matched segment length distribution via KL divergence minimization\n",
    "- Matched orientation distribution\n",
    "- Optimized node density, average degree, segment length metrics\n",
    "- 100 iterations per network with temperature cooling\n",
    "\n",
    "**Operations applied:**\n",
    "- Node position perturbations\n",
    "- Edge additions (between nearby nodes)\n",
    "- Edge removals (maintaining connectivity)\n",
    "\n",
    "**Next Steps:**\n",
    "1. Step 6: Rank optimized networks and select best\n",
    "2. Step 7: Generate optimized buildings for selected network\n",
    "3. Step 8: Export to GeoJSON/Shapefile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}