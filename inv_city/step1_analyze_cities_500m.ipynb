{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Analyze Real Cities (500Ã—500m)\n",
    "## Extract Urban Metrics & Building Block Library\n",
    "\n",
    "**Goal**: Analyze three 500Ã—500m urban areas to extract:\n",
    "- Space syntax metrics (nodes, edges, districts, landmarks, barriers)\n",
    "- Building geometry distributions\n",
    "- Reusable building block library\n",
    "\n",
    "**Cities**:\n",
    "1. Hanoi, Vietnam (21.0230Â°N, 105.8560Â°E) - Dense, organic layout\n",
    "2. Brussels, Belgium (50.8477Â°N, 4.3572Â°E) - European historic core\n",
    "3. Marrakech, Morocco (31.623811Â°N, -7.988662Â°W) - Compact medina\n",
    "\n",
    "**Outputs**:\n",
    "- GeoJSON files (nodes, edges, buildings, districts, blocks)\n",
    "- JSON metrics file (urban_metrics.json)\n",
    "- Building block library (building_blocks_library.json)\n",
    "- Visualizations (PNG + SVG) with base maps and clear labels\n",
    "- Metrics summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib import cm\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon, box, MultiLineString\n",
    "from shapely.ops import polygonize, unary_union, nearest_points, linemerge\n",
    "from shapely.affinity import rotate, scale, translate\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure OSMnx\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.log_console = False\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CITIES = {\n",
    "    'hanoi': {\n",
    "        'name': 'Hanoi, Vietnam',\n",
    "        'coords': (21.0230, 105.8560),\n",
    "        'color': '#FF6B6B'  # Red\n",
    "    },\n",
    "    'brussels': {\n",
    "        'name': 'Brussels, Belgium',\n",
    "        'coords': (50.8477, 4.3572),\n",
    "        'color': '#4ECDC4'  # Teal\n",
    "    },\n",
    "    'marrakech': {\n",
    "        'name': 'Marrakech, Morocco',\n",
    "        'coords': (31.623811, -7.988662),\n",
    "        'color': '#FFE66D'  # Yellow\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analysis parameters (adapted for 500Ã—500m)\n",
    "RADIUS = 250  # meters (to get ~500Ã—500m coverage)\n",
    "REACH_RADII = [200, 300]  # Reduced from 400/600 for small scale\n",
    "LOCAL_LANDMARK_RADIUS = 300  # Reduced from 1500m\n",
    "MIN_BLOCK_AREA = 500  # mÂ²\n",
    "MAX_BLOCK_AREA = 10000  # mÂ²\n",
    "BLOCKS_PER_CITY = 35  # Target library size\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "GEOJSON_DIR = OUTPUT_DIR / 'geojson'\n",
    "VIZ_PNG_DIR = OUTPUT_DIR / 'visualizations' / 'png'\n",
    "VIZ_SVG_DIR = OUTPUT_DIR / 'visualizations' / 'svg'\n",
    "METRICS_DIR = OUTPUT_DIR / 'metrics'\n",
    "\n",
    "# Create directories\n",
    "for d in [GEOJSON_DIR, VIZ_PNG_DIR, VIZ_SVG_DIR, METRICS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Configuration complete\")\n",
    "print(f\"  Analyzing {len(CITIES)} cities\")\n",
    "print(f\"  Coverage radius: {RADIUS}m (~{RADIUS*2}Ã—{RADIUS*2}m area)\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for all cities\n",
    "city_data = {}\n",
    "\n",
    "for city_key, city_info in CITIES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Downloading: {city_info['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    lat, lon = city_info['coords']\n",
    "    \n",
    "    try:\n",
    "        # Download street network (walk network includes all accessible roads)\n",
    "        print(f\"  â†’ Street network...\")\n",
    "        G = ox.graph_from_point(\n",
    "            (lat, lon),\n",
    "            dist=RADIUS,\n",
    "            network_type='walk',\n",
    "            simplify=True\n",
    "        )\n",
    "        \n",
    "        # Project to local UTM\n",
    "        G_proj = ox.project_graph(G)\n",
    "        \n",
    "        # Download buildings\n",
    "        print(f\"  â†’ Buildings...\")\n",
    "        buildings = ox.features_from_point(\n",
    "            (lat, lon),\n",
    "            dist=RADIUS,\n",
    "            tags={'building': True}\n",
    "        )\n",
    "        \n",
    "        # Project buildings\n",
    "        buildings_proj = buildings.to_crs(ox.graph_to_gdfs(G_proj, nodes=False).crs)\n",
    "        \n",
    "        # Clean building geometries (keep only Polygons/MultiPolygons)\n",
    "        buildings_proj = buildings_proj[buildings_proj.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n",
    "        \n",
    "        # Convert MultiPolygons to Polygons (take largest)\n",
    "        def get_polygon(geom):\n",
    "            if geom.geom_type == 'Polygon':\n",
    "                return geom\n",
    "            elif geom.geom_type == 'MultiPolygon':\n",
    "                return max(geom.geoms, key=lambda p: p.area)\n",
    "            return geom\n",
    "        \n",
    "        buildings_proj['geometry'] = buildings_proj.geometry.apply(get_polygon)\n",
    "        buildings_proj = buildings_proj[buildings_proj.geometry.type == 'Polygon'].copy()\n",
    "        \n",
    "        # Store data\n",
    "        city_data[city_key] = {\n",
    "            'name': city_info['name'],\n",
    "            'color': city_info['color'],\n",
    "            'coords': (lat, lon),\n",
    "            'graph': G_proj,\n",
    "            'buildings': buildings_proj,\n",
    "            'crs': ox.graph_to_gdfs(G_proj, nodes=False).crs\n",
    "        }\n",
    "        \n",
    "        print(f\"  âœ“ Downloaded:\")\n",
    "        print(f\"    - {G_proj.number_of_nodes()} nodes\")\n",
    "        print(f\"    - {G_proj.number_of_edges()} edges\")\n",
    "        print(f\"    - {len(buildings_proj)} buildings\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error downloading {city_key}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ Data acquisition complete for {len(city_data)} cities\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node Analysis (Centrality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_metrics(G):\n",
    "    \"\"\"\n",
    "    Compute centrality metrics for nodes (intersections)\n",
    "    \"\"\"\n",
    "    print(\"  Computing node centrality metrics...\")\n",
    "    \n",
    "    # Convert to undirected for centrality calculations\n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    # 1. Betweenness Centrality (distance-weighted)\n",
    "    print(\"    - Betweenness (distance)...\")\n",
    "    bc_dist = nx.betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    \n",
    "    # 2. Betweenness Centrality (information - no weight)\n",
    "    print(\"    - Betweenness (information)...\")\n",
    "    bc_info = nx.betweenness_centrality(G_undir, weight=None, normalized=True)\n",
    "    \n",
    "    # 3. Closeness Centrality (distance-weighted)\n",
    "    print(\"    - Closeness...\")\n",
    "    closeness = nx.closeness_centrality(G_undir, distance='length')\n",
    "    \n",
    "    # 4. Reach Centrality (services within radius)\n",
    "    print(\"    - Reach centrality (200m, 300m)...\")\n",
    "    reach_200 = {}\n",
    "    reach_300 = {}\n",
    "    \n",
    "    for node in G_undir.nodes():\n",
    "        lengths = nx.single_source_dijkstra_path_length(G_undir, node, cutoff=200, weight='length')\n",
    "        reach_200[node] = len(lengths)\n",
    "        \n",
    "        lengths = nx.single_source_dijkstra_path_length(G_undir, node, cutoff=300, weight='length')\n",
    "        reach_300[node] = len(lengths)\n",
    "    \n",
    "    # 5. Degree\n",
    "    degree = dict(G_undir.degree())\n",
    "    \n",
    "    # Create GeoDataFrame with metrics\n",
    "    nodes, _ = ox.graph_to_gdfs(G)\n",
    "    nodes['bc_distance'] = nodes.index.map(bc_dist)\n",
    "    nodes['bc_information'] = nodes.index.map(bc_info)\n",
    "    nodes['closeness'] = nodes.index.map(closeness)\n",
    "    nodes['reach_200m'] = nodes.index.map(reach_200)\n",
    "    nodes['reach_300m'] = nodes.index.map(reach_300)\n",
    "    nodes['degree'] = nodes.index.map(degree)\n",
    "    \n",
    "    print(\"  âœ“ Node metrics computed\")\n",
    "    return nodes\n",
    "\n",
    "# Compute for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    city_data[city_key]['nodes'] = compute_node_metrics(city_data[city_key]['graph'])\n",
    "    \n",
    "    # Save GeoJSON\n",
    "    output_file = GEOJSON_DIR / f\"{city_key}_nodes.geojson\"\n",
    "    city_data[city_key]['nodes'].to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"  âœ“ Saved to {output_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Edge Analysis (Street Networks & Blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_metrics(G):\n",
    "    \"\"\"\n",
    "    Compute edge (street segment) metrics\n",
    "    \"\"\"\n",
    "    print(\"  Computing edge metrics...\")\n",
    "    \n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    # 1. Edge betweenness (primal - distance weighted)\n",
    "    print(\"    - Edge betweenness (primal)...\")\n",
    "    edge_bc = nx.edge_betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    \n",
    "    # 2. Create dual graph for angular analysis\n",
    "    print(\"    - Building dual graph...\")\n",
    "    dual_G = nx.Graph()\n",
    "    edge_to_node = {}\n",
    "    \n",
    "    for i, (u, v, k) in enumerate(G_undir.edges(keys=True)):\n",
    "        edge_to_node[(u, v, k)] = i\n",
    "        dual_G.add_node(i, primal_edge=(u, v, k))\n",
    "    \n",
    "    for node in G_undir.nodes():\n",
    "        incident_edges = list(G_undir.edges(node, keys=True))\n",
    "        for i in range(len(incident_edges)):\n",
    "            for j in range(i+1, len(incident_edges)):\n",
    "                e1 = incident_edges[i]\n",
    "                e2 = incident_edges[j]\n",
    "                e1_norm = tuple(sorted([e1[0], e1[1]])) + (e1[2],)\n",
    "                e2_norm = tuple(sorted([e2[0], e2[1]])) + (e2[2],)\n",
    "                \n",
    "                if e1_norm in edge_to_node and e2_norm in edge_to_node:\n",
    "                    dual_G.add_edge(edge_to_node[e1_norm], edge_to_node[e2_norm])\n",
    "    \n",
    "    # 3. Angular betweenness (dual graph)\n",
    "    print(\"    - Angular betweenness (dual)...\")\n",
    "    dual_bc = nx.betweenness_centrality(dual_G, weight=None, normalized=True) if dual_G.number_of_edges() > 0 else {}\n",
    "    \n",
    "    angular_bc = {}\n",
    "    for dual_node, bc_val in dual_bc.items():\n",
    "        primal_edge = dual_G.nodes[dual_node].get('primal_edge')\n",
    "        if primal_edge:\n",
    "            angular_bc[primal_edge] = bc_val\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    _, edges = ox.graph_to_gdfs(G)\n",
    "    edges['edge_bc'] = edges.index.map(lambda x: edge_bc.get((x[0], x[1]), 0))\n",
    "    edges['angular_bc'] = edges.index.map(lambda x: angular_bc.get(x, 0))\n",
    "    \n",
    "    print(\"  âœ“ Edge metrics computed\")\n",
    "    return edges, dual_G\n",
    "\n",
    "# Compute for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    edges, dual_graph = compute_edge_metrics(city_data[city_key]['graph'])\n",
    "    city_data[city_key]['edges'] = edges\n",
    "    city_data[city_key]['dual_graph'] = dual_graph\n",
    "    \n",
    "    output_file = GEOJSON_DIR / f\"{city_key}_edges.geojson\"\n",
    "    edges.to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"  âœ“ Saved to {output_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Extract blocks using polygonize with proper geometry handling\n",
    "def extract_blocks(edges_gdf, buffer_dist=0.1):\n",
    "    \"\"\"\n",
    "    Extract urban blocks by polygonizing street network\n",
    "    FIX: Added geometry cleaning and buffering to ensure closed polygons\n",
    "    \"\"\"\n",
    "    print(\"  Extracting blocks...\")\n",
    "    \n",
    "    try:\n",
    "        # Get all line geometries\n",
    "        lines = []\n",
    "        for geom in edges_gdf.geometry:\n",
    "            if geom.geom_type == 'LineString':\n",
    "                lines.append(geom)\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                lines.extend(list(geom.geoms))\n",
    "        \n",
    "        if not lines:\n",
    "            print(\"  âš  No valid line geometries found\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'area', 'perimeter', 'compactness', 'aspect_ratio', 'block_id'], crs=edges_gdf.crs)\n",
    "        \n",
    "        # Method 1: Direct polygonize\n",
    "        print(\"    - Attempting direct polygonize...\")\n",
    "        polygons = list(polygonize(lines))\n",
    "        \n",
    "        # Method 2: If no polygons, try buffering slightly to close gaps\n",
    "        if len(polygons) == 0:\n",
    "            print(\"    - Direct polygonize failed, trying with buffered lines...\")\n",
    "            buffered_lines = [line.buffer(buffer_dist) for line in lines]\n",
    "            merged = unary_union(buffered_lines)\n",
    "            \n",
    "            # Extract exterior rings as potential blocks\n",
    "            if hasattr(merged, 'geoms'):\n",
    "                for geom in merged.geoms:\n",
    "                    if geom.geom_type == 'Polygon':\n",
    "                        # Erode back to get original size\n",
    "                        poly = geom.buffer(-buffer_dist)\n",
    "                        if poly.is_valid and not poly.is_empty and poly.geom_type == 'Polygon':\n",
    "                            polygons.append(poly)\n",
    "            elif merged.geom_type == 'Polygon':\n",
    "                poly = merged.buffer(-buffer_dist)\n",
    "                if poly.is_valid and not poly.is_empty and poly.geom_type == 'Polygon':\n",
    "                    polygons.append(poly)\n",
    "        \n",
    "        if len(polygons) == 0:\n",
    "            print(\"  âš  No blocks found after both methods\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'area', 'perimeter', 'compactness', 'aspect_ratio', 'block_id'], crs=edges_gdf.crs)\n",
    "        \n",
    "        print(f\"    - Found {len(polygons)} raw polygons\")\n",
    "        \n",
    "        # Create GeoDataFrame\n",
    "        blocks_gdf = gpd.GeoDataFrame(geometry=polygons, crs=edges_gdf.crs)\n",
    "        \n",
    "        # Compute metrics\n",
    "        blocks_gdf['area'] = blocks_gdf.geometry.area\n",
    "        blocks_gdf['perimeter'] = blocks_gdf.geometry.length\n",
    "        blocks_gdf['compactness'] = (4 * np.pi * blocks_gdf['area']) / (blocks_gdf['perimeter'] ** 2)\n",
    "        \n",
    "        # Filter by size\n",
    "        blocks_gdf = blocks_gdf[\n",
    "            (blocks_gdf['area'] >= MIN_BLOCK_AREA) & \n",
    "            (blocks_gdf['area'] <= MAX_BLOCK_AREA)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(blocks_gdf) == 0:\n",
    "            print(f\"  âš  No blocks within size range ({MIN_BLOCK_AREA}-{MAX_BLOCK_AREA} mÂ²)\")\n",
    "            return gpd.GeoDataFrame(columns=['geometry', 'area', 'perimeter', 'compactness', 'aspect_ratio', 'block_id'], crs=edges_gdf.crs)\n",
    "        \n",
    "        # Compute aspect ratio\n",
    "        aspect_ratios = []\n",
    "        for geom in blocks_gdf.geometry:\n",
    "            try:\n",
    "                mbr = geom.minimum_rotated_rectangle\n",
    "                coords = list(mbr.exterior.coords)\n",
    "                side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "                side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "                aspect = max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0\n",
    "                aspect_ratios.append(aspect)\n",
    "            except:\n",
    "                aspect_ratios.append(1.0)\n",
    "        \n",
    "        blocks_gdf['aspect_ratio'] = aspect_ratios\n",
    "        blocks_gdf['block_id'] = [f\"block_{i:03d}\" for i in range(len(blocks_gdf))]\n",
    "        \n",
    "        print(f\"  âœ“ Extracted {len(blocks_gdf)} valid blocks\")\n",
    "        return blocks_gdf\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error extracting blocks: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return gpd.GeoDataFrame(columns=['geometry', 'area', 'perimeter', 'compactness', 'aspect_ratio', 'block_id'], crs=edges_gdf.crs)\n",
    "\n",
    "# Extract blocks for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    blocks = extract_blocks(city_data[city_key]['edges'])\n",
    "    city_data[city_key]['blocks'] = blocks\n",
    "    \n",
    "    if len(blocks) > 0:\n",
    "        output_file = GEOJSON_DIR / f\"{city_key}_blocks.geojson\"\n",
    "        blocks.to_file(output_file, driver='GeoJSON')\n",
    "        print(f\"  âœ“ Saved to {output_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. District Analysis (Community Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install community detection library if needed\n",
    "try:\n",
    "    import community.community_louvain as community_louvain\n",
    "except ImportError:\n",
    "    try:\n",
    "        import community as community_louvain\n",
    "    except ImportError:\n",
    "        print(\"Installing python-louvain...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-louvain\"])\n",
    "        import community.community_louvain as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: District detection with proper weight handling\n",
    "def detect_districts(G, method='distance'):\n",
    "    \"\"\"\n",
    "    Detect urban districts using community detection\n",
    "    FIX: Properly handle weight parameter as string attribute name\n",
    "    \"\"\"\n",
    "    print(f\"    - Detecting districts ({method})...\")\n",
    "    \n",
    "    try:\n",
    "        G_undir = G.to_undirected()\n",
    "        \n",
    "        # Create a simple graph (remove parallel edges)\n",
    "        G_simple = nx.Graph()\n",
    "        for u, v, data in G_undir.edges(data=True):\n",
    "            if not G_simple.has_edge(u, v):\n",
    "                G_simple.add_edge(u, v, **data)\n",
    "        \n",
    "        # Community detection based on method\n",
    "        if method == 'distance':\n",
    "            # Use 'length' attribute as weight\n",
    "            partition = community_louvain.best_partition(G_simple, weight='length')\n",
    "        else:\n",
    "            # No weight (topological or angular)\n",
    "            partition = community_louvain.best_partition(G_simple, weight=None)\n",
    "        \n",
    "        return partition\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      âœ— Error: {e}\")\n",
    "        # Return single community as fallback\n",
    "        return {node: 0 for node in G.nodes()}\n",
    "\n",
    "def partition_to_geodataframe(nodes_gdf, partition):\n",
    "    \"\"\"Convert node partition to GeoDataFrame\"\"\"\n",
    "    nodes_copy = nodes_gdf.copy()\n",
    "    nodes_copy['district'] = nodes_copy.index.map(partition)\n",
    "    return nodes_copy\n",
    "\n",
    "# Detect districts for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    \n",
    "    G = city_data[city_key]['graph']\n",
    "    nodes = city_data[city_key]['nodes']\n",
    "    \n",
    "    partitions = {}\n",
    "    for method in ['distance', 'angular', 'topological']:\n",
    "        partition = detect_districts(G, method=method)\n",
    "        partitions[method] = partition\n",
    "        \n",
    "        nodes_districts = partition_to_geodataframe(nodes, partition)\n",
    "        \n",
    "        output_file = GEOJSON_DIR / f\"{city_key}_districts_{method}.geojson\"\n",
    "        nodes_districts.to_file(output_file, driver='GeoJSON')\n",
    "        \n",
    "        num_districts = len(set(partition.values()))\n",
    "        print(f\"      {method}: {num_districts} districts\")\n",
    "    \n",
    "    city_data[city_key]['partitions'] = partitions\n",
    "    print(f\"  âœ“ District detection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Landmark Analysis (Building Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Landmark scoring with proper NaN handling\n",
    "def safe_normalize(series, default=0.5):\n",
    "    \"\"\"Safely normalize a series, handling NaN and min==max cases\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    \n",
    "    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:\n",
    "        return pd.Series([default] * len(series), index=series.index)\n",
    "    \n",
    "    normalized = (series - min_val) / (max_val - min_val)\n",
    "    return normalized.fillna(default)\n",
    "\n",
    "def compute_building_landmark_scores(buildings_gdf, edges_gdf):\n",
    "    \"\"\"\n",
    "    Compute landmark scores for buildings\n",
    "    FIX: Added safe normalization to prevent NaN values\n",
    "    \"\"\"\n",
    "    print(\"  Computing landmark scores...\")\n",
    "    \n",
    "    buildings = buildings_gdf.copy()\n",
    "    \n",
    "    # 1. Structural Score (area-based)\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    buildings['s_area'] = safe_normalize(buildings['area'], default=0.5)\n",
    "    \n",
    "    # 2D visibility: Distance to nearest street\n",
    "    print(\"    - Computing visibility...\")\n",
    "    street_union = unary_union(edges_gdf.geometry)\n",
    "    buildings['dist_to_street'] = buildings.geometry.apply(\n",
    "        lambda geom: geom.distance(street_union)\n",
    "    )\n",
    "    \n",
    "    # Inverse distance = visibility\n",
    "    max_dist = buildings['dist_to_street'].max()\n",
    "    if max_dist > 0:\n",
    "        buildings['s_visibility'] = 1 - (buildings['dist_to_street'] / max_dist)\n",
    "    else:\n",
    "        buildings['s_visibility'] = 0.5\n",
    "    buildings['s_visibility'] = buildings['s_visibility'].fillna(0.5)\n",
    "    \n",
    "    # Structural score\n",
    "    buildings['structural_score'] = 0.6 * buildings['s_area'] + 0.4 * buildings['s_visibility']\n",
    "    buildings['structural_score'] = buildings['structural_score'].fillna(0.5)\n",
    "    \n",
    "    # 2. Visual Score (height)\n",
    "    if 'height' in buildings.columns:\n",
    "        buildings['height'] = pd.to_numeric(buildings['height'], errors='coerce')\n",
    "        buildings['visual_score'] = safe_normalize(buildings['height'], default=0.5)\n",
    "    else:\n",
    "        buildings['visual_score'] = 0.5\n",
    "    \n",
    "    # 3. Cultural Score\n",
    "    cultural_tags = ['historic', 'tourism', 'amenity', 'heritage']\n",
    "    buildings['cultural_score'] = 0.0\n",
    "    for tag in cultural_tags:\n",
    "        if tag in buildings.columns:\n",
    "            buildings.loc[buildings[tag].notna(), 'cultural_score'] += 0.25\n",
    "    buildings['cultural_score'] = buildings['cultural_score'].clip(0, 1)\n",
    "    \n",
    "    # 4. Pragmatic Score\n",
    "    important_uses = ['school', 'hospital', 'university', 'museum', 'church', 'mosque', 'temple', 'government']\n",
    "    buildings['pragmatic_score'] = 0.0\n",
    "    \n",
    "    for col in ['building', 'amenity', 'tourism']:\n",
    "        if col in buildings.columns:\n",
    "            for use in important_uses:\n",
    "                mask = buildings[col].astype(str).str.contains(use, case=False, na=False)\n",
    "                buildings.loc[mask, 'pragmatic_score'] = 1.0\n",
    "    \n",
    "    # 5. Global Landmark Score\n",
    "    buildings['global_score'] = (\n",
    "        0.4 * buildings['structural_score'] +\n",
    "        0.2 * buildings['visual_score'] +\n",
    "        0.2 * buildings['cultural_score'] +\n",
    "        0.2 * buildings['pragmatic_score']\n",
    "    )\n",
    "    buildings['global_score'] = buildings['global_score'].fillna(0.5)\n",
    "    \n",
    "    print(\"  âœ“ Landmark scores computed\")\n",
    "    return buildings\n",
    "\n",
    "# Compute for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    buildings_scored = compute_building_landmark_scores(\n",
    "        city_data[city_key]['buildings'],\n",
    "        city_data[city_key]['edges']\n",
    "    )\n",
    "    city_data[city_key]['buildings_scored'] = buildings_scored\n",
    "    \n",
    "    output_file = GEOJSON_DIR / f\"{city_key}_buildings.geojson\"\n",
    "    cols_to_save = ['geometry', 'area', 'structural_score', 'visual_score', 'cultural_score', 'pragmatic_score', 'global_score']\n",
    "    cols_to_save = [c for c in cols_to_save if c in buildings_scored.columns]\n",
    "    buildings_scored[cols_to_save].to_file(output_file, driver='GeoJSON')\n",
    "    print(f\"  âœ“ Saved to {output_file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute building geometry metrics\n",
    "def compute_building_geometry_metrics(buildings_gdf, blocks_gdf, edges_gdf):\n",
    "    \"\"\"Compute additional building metrics\"\"\"\n",
    "    print(\"  Computing geometry metrics...\")\n",
    "    \n",
    "    buildings = buildings_gdf.copy()\n",
    "    \n",
    "    # Aspect ratio\n",
    "    aspect_ratios = []\n",
    "    for geom in buildings.geometry:\n",
    "        try:\n",
    "            mbr = geom.minimum_rotated_rectangle\n",
    "            coords = list(mbr.exterior.coords)\n",
    "            side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "            side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "            aspect = max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0\n",
    "            aspect_ratios.append(aspect)\n",
    "        except:\n",
    "            aspect_ratios.append(1.0)\n",
    "    buildings['aspect_ratio'] = aspect_ratios\n",
    "    \n",
    "    # Courtyard frequency\n",
    "    buildings['has_courtyard'] = buildings.geometry.apply(\n",
    "        lambda geom: len(geom.interiors) > 0 if geom.geom_type == 'Polygon' else False\n",
    "    )\n",
    "    courtyard_freq = buildings['has_courtyard'].sum() / len(buildings) if len(buildings) > 0 else 0\n",
    "    \n",
    "    # Setback distance\n",
    "    if 'dist_to_street' not in buildings.columns:\n",
    "        street_union = unary_union(edges_gdf.geometry)\n",
    "        buildings['setback_dist'] = buildings.geometry.apply(lambda geom: geom.distance(street_union))\n",
    "    else:\n",
    "        buildings['setback_dist'] = buildings['dist_to_street']\n",
    "    \n",
    "    # Building coverage ratio\n",
    "    if len(blocks_gdf) > 0:\n",
    "        print(\"    - Computing coverage ratios...\")\n",
    "        blocks = blocks_gdf.copy()\n",
    "        buildings_in_blocks = gpd.sjoin(buildings, blocks, how='left', predicate='within')\n",
    "        block_building_area = buildings_in_blocks.groupby('index_right')['area'].sum()\n",
    "        blocks['building_coverage'] = blocks.index.map(block_building_area).fillna(0) / blocks['area']\n",
    "        blocks['building_count'] = buildings_in_blocks.groupby('index_right').size().reindex(blocks.index, fill_value=0)\n",
    "        avg_coverage = blocks['building_coverage'].mean()\n",
    "    else:\n",
    "        avg_coverage = 0\n",
    "        blocks = blocks_gdf\n",
    "    \n",
    "    print(\"  âœ“ Geometry metrics computed\")\n",
    "    \n",
    "    return buildings, blocks, {\n",
    "        'courtyard_frequency': courtyard_freq,\n",
    "        'avg_coverage': avg_coverage\n",
    "    }\n",
    "\n",
    "# Compute for all cities\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    buildings_geom, blocks_geom, metrics = compute_building_geometry_metrics(\n",
    "        city_data[city_key]['buildings_scored'],\n",
    "        city_data[city_key]['blocks'],\n",
    "        city_data[city_key]['edges']\n",
    "    )\n",
    "    \n",
    "    city_data[city_key]['buildings_scored'] = buildings_geom\n",
    "    city_data[city_key]['blocks'] = blocks_geom\n",
    "    city_data[city_key]['geometry_metrics'] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building Block Library Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Building block library with proper validation\n",
    "def extract_building_block_library(blocks_gdf, buildings_gdf, city_key, target_count=35):\n",
    "    \"\"\"\n",
    "    Extract representative building blocks\n",
    "    FIX: Handle empty blocks gracefully\n",
    "    \"\"\"\n",
    "    print(f\"  Extracting blocks for library...\")\n",
    "    \n",
    "    if len(blocks_gdf) == 0:\n",
    "        print(\"  âš  No blocks available - skipping library extraction\")\n",
    "        return []\n",
    "    \n",
    "    blocks = blocks_gdf.copy().sort_values('area')\n",
    "    \n",
    "    if len(blocks) <= target_count:\n",
    "        selected_blocks = blocks\n",
    "    else:\n",
    "        indices = np.linspace(0, len(blocks)-1, target_count, dtype=int)\n",
    "        selected_blocks = blocks.iloc[indices]\n",
    "    \n",
    "    library = []\n",
    "    \n",
    "    for idx, (block_idx, block_row) in enumerate(selected_blocks.iterrows()):\n",
    "        block_geom = block_row.geometry\n",
    "        block_centroid = block_geom.centroid\n",
    "        \n",
    "        buildings_in_block = buildings_gdf[buildings_gdf.geometry.within(block_geom)]\n",
    "        \n",
    "        buildings_relative = []\n",
    "        for _, bldg in buildings_in_block.iterrows():\n",
    "            translated = translate(\n",
    "                bldg.geometry,\n",
    "                xoff=-block_centroid.x,\n",
    "                yoff=-block_centroid.y\n",
    "            )\n",
    "            buildings_relative.append({\n",
    "                'type': 'Polygon',\n",
    "                'coordinates': [list(translated.exterior.coords)]\n",
    "            })\n",
    "        \n",
    "        block_relative = translate(block_geom, xoff=-block_centroid.x, yoff=-block_centroid.y)\n",
    "        \n",
    "        library_entry = {\n",
    "            'block_id': f\"{city_key}_block_{idx:03d}\",\n",
    "            'city': city_key,\n",
    "            'area': float(block_row['area']),\n",
    "            'perimeter': float(block_row['perimeter']),\n",
    "            'compactness': float(block_row['compactness']),\n",
    "            'aspect_ratio': float(block_row['aspect_ratio']),\n",
    "            'building_count': len(buildings_in_block),\n",
    "            'building_coverage': float(block_row.get('building_coverage', 0)),\n",
    "            'block_boundary': {\n",
    "                'type': 'Polygon',\n",
    "                'coordinates': [list(block_relative.exterior.coords)]\n",
    "            },\n",
    "            'buildings': buildings_relative\n",
    "        }\n",
    "        \n",
    "        library.append(library_entry)\n",
    "    \n",
    "    print(f\"  âœ“ Extracted {len(library)} blocks\")\n",
    "    return library\n",
    "\n",
    "# Extract for all cities\n",
    "all_blocks_library = []\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    library = extract_building_block_library(\n",
    "        city_data[city_key]['blocks'],\n",
    "        city_data[city_key]['buildings_scored'],\n",
    "        city_key,\n",
    "        target_count=BLOCKS_PER_CITY\n",
    "    )\n",
    "    all_blocks_library.extend(library)\n",
    "    city_data[city_key]['library'] = library\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ Total library size: {len(all_blocks_library)} blocks\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save library\n",
    "if len(all_blocks_library) > 0:\n",
    "    library_file = METRICS_DIR / 'building_blocks_library.json'\n",
    "    with open(library_file, 'w') as f:\n",
    "        json.dump(all_blocks_library, f, indent=2)\n",
    "    print(f\"âœ“ Saved library to {library_file.name}\")\n",
    "else:\n",
    "    print(\"âš  No blocks in library - skipping JSON export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metrics Aggregation & JSON Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution(values, bins=20):\n",
    "    \"\"\"Compute histogram distribution\"\"\"\n",
    "    if len(values) == 0:\n",
    "        return {'bins': [], 'counts': [], 'mean': 0, 'median': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "    \n",
    "    hist, bin_edges = np.histogram(values, bins=bins)\n",
    "    \n",
    "    return {\n",
    "        'bins': bin_edges.tolist(),\n",
    "        'counts': hist.tolist(),\n",
    "        'mean': float(np.mean(values)),\n",
    "        'median': float(np.median(values)),\n",
    "        'std': float(np.std(values)),\n",
    "        'min': float(np.min(values)),\n",
    "        'max': float(np.max(values))\n",
    "    }\n",
    "\n",
    "# FIXED: Aggregate metrics with proper error handling\n",
    "urban_metrics = {}\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\nAggregating metrics for {city_data[city_key]['name']}...\")\n",
    "    \n",
    "    nodes = city_data[city_key]['nodes']\n",
    "    edges = city_data[city_key]['edges']\n",
    "    blocks = city_data[city_key]['blocks']\n",
    "    buildings = city_data[city_key]['buildings_scored']\n",
    "    partitions = city_data[city_key].get('partitions', {})\n",
    "    geom_metrics = city_data[city_key].get('geometry_metrics', {})\n",
    "    \n",
    "    degree_dist = nodes['degree'].value_counts().to_dict()\n",
    "    degree_dist = {int(k): int(v) for k, v in degree_dist.items()}\n",
    "    \n",
    "    urban_metrics[city_key] = {\n",
    "        'name': city_data[city_key]['name'],\n",
    "        'nodes': {\n",
    "            'total_count': len(nodes),\n",
    "            'avg_degree': float(nodes['degree'].mean()),\n",
    "            'degree_distribution': degree_dist,\n",
    "            'bc_distance': compute_distribution(nodes['bc_distance'].values),\n",
    "            'bc_information': compute_distribution(nodes['bc_information'].values),\n",
    "            'reach_200m': compute_distribution(nodes['reach_200m'].values),\n",
    "            'reach_300m': compute_distribution(nodes['reach_300m'].values)\n",
    "        },\n",
    "        'edges': {\n",
    "            'total_count': len(edges),\n",
    "            'total_length_km': float(edges['length'].sum() / 1000),\n",
    "            'density_km_per_km2': float((edges['length'].sum() / 1000) / 0.25),\n",
    "            'segment_length_distribution': compute_distribution(edges['length'].values),\n",
    "            'angular_bc_distribution': compute_distribution(edges['angular_bc'].values)\n",
    "        },\n",
    "        'blocks': {\n",
    "            'total_count': len(blocks),\n",
    "            'area_distribution': compute_distribution(blocks['area'].values) if len(blocks) > 0 else {},\n",
    "            'compactness_distribution': compute_distribution(blocks['compactness'].values) if len(blocks) > 0 else {},\n",
    "            'aspect_ratio_distribution': compute_distribution(blocks['aspect_ratio'].values) if len(blocks) > 0 else {}\n",
    "        },\n",
    "        'buildings': {\n",
    "            'total_count': len(buildings),\n",
    "            'area_distribution': compute_distribution(buildings['area'].values),\n",
    "            'aspect_ratio_distribution': compute_distribution(buildings['aspect_ratio'].values),\n",
    "            'setback_distribution': compute_distribution(buildings['setback_dist'].values),\n",
    "            'avg_coverage_ratio': float(geom_metrics.get('avg_coverage', 0)),\n",
    "            'courtyard_frequency': float(geom_metrics.get('courtyard_frequency', 0))\n",
    "        },\n",
    "        'districts': {\n",
    "            'count_distance': len(set(partitions.get('distance', {}).values())) if 'distance' in partitions else 0,\n",
    "            'count_angular': len(set(partitions.get('angular', {}).values())) if 'angular' in partitions else 0,\n",
    "            'count_topological': len(set(partitions.get('topological', {}).values())) if 'topological' in partitions else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Save to JSON\n",
    "metrics_file = METRICS_DIR / 'urban_metrics.json'\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump({'urban_metrics': urban_metrics}, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ“ Metrics saved to {metrics_file.name}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. NEW: Metrics Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Create comprehensive metrics summary table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ðŸ“Š COMPREHENSIVE METRICS SUMMARY TABLE\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "metrics_data = []\n",
    "\n",
    "metric_definitions = [\n",
    "    ('Total Nodes', lambda m: m['nodes']['total_count'], ''),\n",
    "    ('Avg Node Degree', lambda m: m['nodes']['avg_degree'], '.2f'),\n",
    "    ('Total Edges', lambda m: m['edges']['total_count'], ''),\n",
    "    ('Total Street Length', lambda m: m['edges']['total_length_km'], '.2f km'),\n",
    "    ('Street Density', lambda m: m['edges']['density_km_per_km2'], '.1f km/kmÂ²'),\n",
    "    ('Avg Segment Length', lambda m: m['edges']['segment_length_distribution']['mean'], '.1f m'),\n",
    "    ('Median Segment Length', lambda m: m['edges']['segment_length_distribution']['median'], '.1f m'),\n",
    "    ('Total Blocks', lambda m: m['blocks']['total_count'], ''),\n",
    "    ('Avg Block Area', lambda m: m['blocks']['area_distribution'].get('mean', 0), '.0f mÂ²'),\n",
    "    ('Median Block Area', lambda m: m['blocks']['area_distribution'].get('median', 0), '.0f mÂ²'),\n",
    "    ('Avg Block Compactness', lambda m: m['blocks']['compactness_distribution'].get('mean', 0), '.2f'),\n",
    "    ('Total Buildings', lambda m: m['buildings']['total_count'], ''),\n",
    "    ('Avg Building Area', lambda m: m['buildings']['area_distribution']['mean'], '.0f mÂ²'),\n",
    "    ('Median Building Area', lambda m: m['buildings']['area_distribution']['median'], '.0f mÂ²'),\n",
    "    ('Avg Building Aspect Ratio', lambda m: m['buildings']['aspect_ratio_distribution']['mean'], '.2f'),\n",
    "    ('Avg Setback Distance', lambda m: m['buildings']['setback_distribution']['mean'], '.2f m'),\n",
    "    ('Building Coverage Ratio', lambda m: m['buildings']['avg_coverage_ratio'] * 100, '.1f%'),\n",
    "    ('Courtyard Frequency', lambda m: m['buildings']['courtyard_frequency'] * 100, '.1f%'),\n",
    "    ('Districts (Distance)', lambda m: m['districts']['count_distance'], ''),\n",
    "    ('Districts (Angular)', lambda m: m['districts']['count_angular'], ''),\n",
    "    ('Districts (Topological)', lambda m: m['districts']['count_topological'], ''),\n",
    "]\n",
    "\n",
    "# Build table\n",
    "table_data = []\n",
    "for metric_name, metric_func, fmt in metric_definitions:\n",
    "    row = {'Metric': metric_name}\n",
    "    for city_key in city_data.keys():\n",
    "        try:\n",
    "            value = metric_func(urban_metrics[city_key])\n",
    "            if fmt:\n",
    "                if 'kmÂ²' in fmt or 'km' in fmt or 'mÂ²' in fmt or 'm' in fmt or '%' in fmt:\n",
    "                    # Extract format spec\n",
    "                    format_spec = fmt.split()[0]\n",
    "                    unit = ' '.join(fmt.split()[1:])\n",
    "                    row[city_data[city_key]['name']] = f\"{value:{format_spec}} {unit}\".strip()\n",
    "                else:\n",
    "                    row[city_data[city_key]['name']] = f\"{value:{fmt}}\"\n",
    "            else:\n",
    "                row[city_data[city_key]['name']] = str(int(value)) if isinstance(value, (int, float)) else str(value)\n",
    "        except:\n",
    "            row[city_data[city_key]['name']] = 'N/A'\n",
    "    table_data.append(row)\n",
    "\n",
    "metrics_df = pd.DataFrame(table_data)\n",
    "metrics_df = metrics_df.set_index('Metric')\n",
    "\n",
    "# Display table\n",
    "print(\"\\n\")\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "csv_file = METRICS_DIR / 'metrics_summary_table.csv'\n",
    "metrics_df.to_csv(csv_file)\n",
    "print(f\"\\nâœ“ Saved metrics table to {csv_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualizations (IMPROVED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 NEW: Base Maps (Roads + Buildings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: Base maps showing roads + buildings\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), facecolor='white')\n",
    "\n",
    "for idx, city_key in enumerate(city_data.keys()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    edges = city_data[city_key]['edges']\n",
    "    buildings = city_data[city_key]['buildings_scored']\n",
    "    \n",
    "    # Plot buildings in gray\n",
    "    buildings.plot(ax=ax, color='#CCCCCC', edgecolor='#666666', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Plot roads in black\n",
    "    edges.plot(ax=ax, color='black', linewidth=1.5)\n",
    "    \n",
    "    ax.set_title(f\"{city_data[city_key]['name']}\\n{len(buildings)} buildings, {len(edges)} road segments\", \n",
    "                 fontsize=14, color='black', pad=15)\n",
    "    ax.set_xlabel('Easting (m)', fontsize=10)\n",
    "    ax.set_ylabel('Northing (m)', fontsize=10)\n",
    "    ax.tick_params(labelsize=8)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.suptitle('Base Maps: Urban Form (500Ã—500m)', fontsize=20, color='black', y=0.98)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(VIZ_PNG_DIR / 'base_maps.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.savefig(VIZ_SVG_DIR / 'base_maps.svg', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: base_maps (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 TIER 1: Comparative Street Network Betweenness Maps (IMPROVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Betweenness maps with better labels and colorbar\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), facecolor='#1a1a1a')\n",
    "\n",
    "for idx, city_key in enumerate(city_data.keys()):\n",
    "    ax = axes[idx]\n",
    "    edges = city_data[city_key]['edges']\n",
    "    \n",
    "    # Plot edges colored by angular betweenness\n",
    "    edges.plot(\n",
    "        ax=ax,\n",
    "        column='angular_bc',\n",
    "        cmap='YlOrRd',\n",
    "        linewidth=2,\n",
    "        legend=False,\n",
    "        vmin=0,\n",
    "        vmax=edges['angular_bc'].max()\n",
    "    )\n",
    "    \n",
    "    ax.set_title(city_data[city_key]['name'], fontsize=20, color='white', pad=20)\n",
    "    ax.set_xlabel('Easting (m)', fontsize=12, color='white')\n",
    "    ax.set_ylabel('Northing (m)', fontsize=12, color='white')\n",
    "    ax.tick_params(colors='white', labelsize=10)\n",
    "    ax.set_facecolor('#1a1a1a')\n",
    "\n",
    "# Add shared colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "sm = plt.cm.ScalarMappable(cmap='YlOrRd', norm=Normalize(vmin=0, vmax=0.1))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "cbar.set_label('Angular Betweenness Centrality', fontsize=14, color='white')\n",
    "cbar.ax.tick_params(colors='white', labelsize=10)\n",
    "\n",
    "plt.suptitle('Angular Betweenness Centrality: Urban Movement Spines', \n",
    "             fontsize=24, color='white', y=0.98)\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "\n",
    "plt.savefig(VIZ_PNG_DIR / 'tier1_betweenness_comparison.png', dpi=300, facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'tier1_betweenness_comparison.svg', facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: tier1_betweenness_comparison (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 TIER 1: Building Block Library (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Block Library visualization\n",
    "if len(all_blocks_library) > 0:\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15), facecolor='white')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    selected_blocks = []\n",
    "    for city_key in city_data.keys():\n",
    "        library = city_data[city_key]['library']\n",
    "        if len(library) >= 4:\n",
    "            indices = [0, len(library)//3, 2*len(library)//3, -1]\n",
    "            selected_blocks.extend([library[i] for i in indices])\n",
    "    \n",
    "    for idx, block_data in enumerate(selected_blocks[:12]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        block_poly = Polygon(block_data['block_boundary']['coordinates'][0])\n",
    "        x, y = block_poly.exterior.xy\n",
    "        ax.fill(x, y, color='#f0f0f0', edgecolor='black', linewidth=1)\n",
    "        \n",
    "        for bldg in block_data['buildings']:\n",
    "            bldg_poly = Polygon(bldg['coordinates'][0])\n",
    "            x, y = bldg_poly.exterior.xy\n",
    "            ax.fill(x, y, color='black')\n",
    "        \n",
    "        ax.set_title(\n",
    "            f\"{block_data['city'].upper()}\\n\"\n",
    "            f\"{block_data['area']:.0f} mÂ² | \"\n",
    "            f\"Coverage: {block_data['building_coverage']*100:.0f}% | \"\n",
    "            f\"AR: {block_data['aspect_ratio']:.1f}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "        \n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for idx in range(len(selected_blocks), 12):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Building Block Library: Urban DNA Samples', fontsize=24, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(VIZ_PNG_DIR / 'tier1_block_library.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(VIZ_SVG_DIR / 'tier1_block_library.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ“ Saved: tier1_block_library (PNG + SVG)\")\n",
    "else:\n",
    "    print(\"âš  Skipping block library visualization (no blocks available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 IMPROVED: Comparative Histograms with Clear Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Comparative histograms with better formatting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), facecolor='white')\n",
    "\n",
    "distributions = [\n",
    "    ('edges', 'segment_length_distribution', 'Street Segment Length', 'Length (meters)'),\n",
    "    ('blocks', 'area_distribution', 'Urban Block Area', 'Area (mÂ²)'),\n",
    "    ('buildings', 'area_distribution', 'Building Footprint Area', 'Area (mÂ²)'),\n",
    "    ('buildings', 'aspect_ratio_distribution', 'Building Aspect Ratio', 'Aspect Ratio (length/width)')\n",
    "]\n",
    "\n",
    "for idx, (category, metric_key, title, xlabel) in enumerate(distributions):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    for city_key in city_data.keys():\n",
    "        metric = urban_metrics[city_key][category].get(metric_key, {})\n",
    "        if 'bins' in metric and len(metric['bins']) > 1:\n",
    "            bin_centers = [(metric['bins'][i] + metric['bins'][i+1])/2 for i in range(len(metric['bins'])-1)]\n",
    "            ax.plot(\n",
    "                bin_centers,\n",
    "                metric['counts'],\n",
    "                label=f\"{city_data[city_key]['name']} (Î¼={metric['mean']:.1f})\",\n",
    "                color=city_data[city_key]['color'],\n",
    "                linewidth=2.5,\n",
    "                alpha=0.8\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel(xlabel, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency (count)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=15, fontweight='bold', pad=15)\n",
    "    ax.legend(fontsize=11, loc='best', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.tick_params(labelsize=10)\n",
    "\n",
    "plt.suptitle('Comparative Distributions: Urban Pattern Analysis (500Ã—500m)', \n",
    "             fontsize=20, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(VIZ_PNG_DIR / 'comparative_histograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'comparative_histograms.svg', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: comparative_histograms (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 IMPROVED: District Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: District maps with labels\n",
    "fig, axes = plt.subplots(3, 3, figsize=(22, 22), facecolor='#1a1a1a')\n",
    "\n",
    "methods = ['distance', 'angular', 'topological']\n",
    "method_titles = ['Distance-Based Partition', 'Angular-Based Partition', 'Topological Partition']\n",
    "\n",
    "for row, city_key in enumerate(city_data.keys()):\n",
    "    for col, method in enumerate(methods):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        partition_file = GEOJSON_DIR / f\"{city_key}_districts_{method}.geojson\"\n",
    "        nodes_districts = gpd.read_file(partition_file)\n",
    "        edges = city_data[city_key]['edges']\n",
    "        \n",
    "        edges.plot(ax=ax, color='#333333', linewidth=1.2, alpha=0.6)\n",
    "        nodes_districts.plot(\n",
    "            ax=ax,\n",
    "            column='district',\n",
    "            cmap='tab20',\n",
    "            markersize=60,\n",
    "            legend=False,\n",
    "            alpha=0.9\n",
    "        )\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(method_titles[col], fontsize=16, color='white', pad=12, fontweight='bold')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(city_data[city_key]['name'], fontsize=15, color='white', \n",
    "                         rotation=90, labelpad=20, fontweight='bold')\n",
    "        \n",
    "        # Add district count\n",
    "        num_districts = len(set(nodes_districts['district']))\n",
    "        ax.text(0.05, 0.95, f\"{num_districts} districts\", \n",
    "               transform=ax.transAxes, fontsize=12, color='white',\n",
    "               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "        \n",
    "        ax.axis('off')\n",
    "        ax.set_facecolor('#1a1a1a')\n",
    "\n",
    "plt.suptitle('District Identification: Community Detection Methods', \n",
    "             fontsize=24, color='white', y=0.98, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(VIZ_PNG_DIR / 'tier2_districts.png', dpi=300, facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'tier2_districts.svg', facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: tier2_districts (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 IMPROVED: Landmark Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPROVED: Landmark maps with colorbars and labels\n",
    "fig, axes = plt.subplots(2, 3, figsize=(24, 16), facecolor='#1a1a1a')\n",
    "\n",
    "score_types = ['structural_score', 'global_score']\n",
    "score_titles = ['Structural Landmark Score', 'Global Landmark Score']\n",
    "\n",
    "for row, score_type in enumerate(score_types):\n",
    "    for col, city_key in enumerate(city_data.keys()):\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        buildings = city_data[city_key]['buildings_scored']\n",
    "        edges = city_data[city_key]['edges']\n",
    "        \n",
    "        edges.plot(ax=ax, color='#333333', linewidth=0.8)\n",
    "        buildings.plot(\n",
    "            ax=ax,\n",
    "            column=score_type,\n",
    "            cmap='hot',\n",
    "            legend=True,\n",
    "            legend_kwds={\n",
    "                'label': score_titles[row],\n",
    "                'shrink': 0.8,\n",
    "                'orientation': 'horizontal',\n",
    "                'pad': 0.05\n",
    "            },\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "        )\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(city_data[city_key]['name'], fontsize=17, color='white', \n",
    "                        pad=15, fontweight='bold')\n",
    "        \n",
    "        ax.axis('off')\n",
    "        ax.set_facecolor('#1a1a1a')\n",
    "\n",
    "plt.suptitle('Landmark Identification: Building Importance Scores (0-1 scale)', \n",
    "             fontsize=24, color='white', y=0.98, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(VIZ_PNG_DIR / 'tier2_landmarks.png', dpi=300, facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'tier2_landmarks.svg', facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Saved: tier2_landmarks (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"âœ“ STEP 1 COMPLETE: URBAN ANALYSIS (500Ã—500m)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nðŸ“ OUTPUTS GENERATED:\")\n",
    "print(f\"\\n  GeoJSON Files ({GEOJSON_DIR}):\")\n",
    "for f in sorted(GEOJSON_DIR.glob('*.geojson')):\n",
    "    print(f\"    - {f.name}\")\n",
    "\n",
    "print(f\"\\n  Metrics ({METRICS_DIR}):\")\n",
    "for f in sorted(METRICS_DIR.glob('*')):\n",
    "    print(f\"    - {f.name}\")\n",
    "\n",
    "print(f\"\\n  Visualizations:\")\n",
    "print(f\"    PNG ({len(list(VIZ_PNG_DIR.glob('*.png')))} files): {VIZ_PNG_DIR}\")\n",
    "print(f\"    SVG ({len(list(VIZ_SVG_DIR.glob('*.svg')))} files): {VIZ_SVG_DIR}\")\n",
    "\n",
    "print(\"\\nðŸ“Š KEY RESULTS:\")\n",
    "for city_key in city_data.keys():\n",
    "    m = urban_metrics[city_key]\n",
    "    print(f\"\\n  {m['name'].upper()}:\")\n",
    "    print(f\"    Nodes: {m['nodes']['total_count']} | Edges: {m['edges']['total_count']} | Blocks: {m['blocks']['total_count']} | Buildings: {m['buildings']['total_count']}\")\n",
    "    print(f\"    Street density: {m['edges']['density_km_per_km2']:.1f} km/kmÂ²\")\n",
    "    print(f\"    Library blocks: {len(city_data[city_key]['library'])}\")\n",
    "\n",
    "print(f\"\\n  TOTAL BUILDING BLOCK LIBRARY: {len(all_blocks_library)} blocks\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ NEXT STEPS (STEP 2):\")\n",
    "print(\"  1. Generate 500Ã—500m road network using tensor field\")\n",
    "print(\"  2. Use segment length distributions from this analysis\")\n",
    "print(\"  3. Optimize for space syntax metrics (betweenness, integration)\")\n",
    "print(\"  4. Place buildings using block library from STEP 1\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"All visualizations include proper axis labels, clear titles, and legends.\")\n",
    "print(\"Both PNG (high-res) and SVG (vector) formats exported for portfolio use.\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
