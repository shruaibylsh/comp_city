{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Analyze Real Cities (500\u00d7500m)\n",
    "## Extract Urban Metrics & Building Footprint Library\n",
    "\n",
    "**Goal**: Analyze three 500\u00d7500m urban areas to extract:\n",
    "- Space syntax metrics (nodes, edges, districts, landmarks)\n",
    "- Building footprint library (individual building shapes)\n",
    "- Building parcels (land use boundaries)\n",
    "- Building geometry distributions\n",
    "\n",
    "**Cities**:\n",
    "1. Hanoi, Vietnam (21.0230\u00b0N, 105.8560\u00b0E) - Dense, organic layout\n",
    "2. Brussels, Belgium (50.8477\u00b0N, 4.3572\u00b0E) - European historic core\n",
    "3. Marrakech, Morocco (31.623811\u00b0N, -7.988662\u00b0W) - Compact medina\n",
    "\n",
    "**Outputs**:\n",
    "- GeoJSON files (nodes, edges, buildings, parcels, districts)\n",
    "- JSON metrics file (urban_metrics.json)\n",
    "- Building footprint library (building_footprint_library.json)\n",
    "- Visualizations (PNG + SVG)\n",
    "- Metrics summary table (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib import cm\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon, box\n",
    "from shapely.ops import unary_union\n",
    "from shapely.affinity import translate\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure OSMnx\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.log_console = False\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCITIES = {\n    'london': {\n        'name': 'London, UK',\n        'coords': (51.5108708294874, -0.1301202729436442),\n        'color': '#E74C3C'\n    },\n    'berlin': {\n        'name': 'Berlin, Germany',\n        'coords': (52.52832783083204, 13.40299924970717),\n        'color': '#3498DB'\n    },\n    'belgrade': {\n        'name': 'Belgrade, Serbia',\n        'coords': (44.81648489551224, 20.462214816208164),\n        'color': '#2ECC71'\n    },\n    'torino': {\n        'name': 'Torino, Italy',\n        'coords': (45.06940684010285, 7.682084193995683),\n        'color': '#F39C12'\n    }\n}\n\n# Analysis parameters (adapted for 500\u00d7500m)\nRADIUS = 250  # meters\nREACH_RADII = [200, 300]\nLOCAL_LANDMARK_RADIUS = 300\nMIN_PARCEL_AREA = 500  # m\u00b2\nMAX_PARCEL_AREA = 10000  # m\u00b2\nFOOTPRINTS_PER_CITY = 35  # Target library size\nLANDMARK_THRESHOLD = 0.4  # Hard threshold for landmark identification (no max limit)\n\n# Output paths\nOUTPUT_DIR = Path('outputs')\nGEOJSON_DIR = OUTPUT_DIR / 'geojson'\nVIZ_PNG_DIR = OUTPUT_DIR / 'visualizations' / 'png'\nVIZ_SVG_DIR = OUTPUT_DIR / 'visualizations' / 'svg'\nMETRICS_DIR = OUTPUT_DIR / 'metrics'\n\nfor d in [GEOJSON_DIR, VIZ_PNG_DIR, VIZ_SVG_DIR, METRICS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nprint(\"\u2713 Configuration complete\")\nprint(f\"  Analyzing {len(CITIES)} cities\")\nprint(f\"  Coverage radius: {RADIUS}m (~{RADIUS*2}\u00d7{RADIUS*2}m)\")\nprint(f\"  Output: {OUTPUT_DIR.absolute()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download data for all cities\ncity_data = {}\n\nfor city_key, city_info in CITIES.items():\n    print(f\"\\n{'='*60}\")\n    print(f\"Downloading: {city_info['name']}\")\n    print(f\"{'='*60}\")\n    \n    lat, lon = city_info['coords']\n    \n    try:\n        # 1. Street network - GET ALL ROADS (all hierarchy levels)\n        print(f\"  \u2192 Street network (all types)...\")\n        G = ox.graph_from_point((lat, lon), dist=RADIUS, network_type='all', simplify=True)\n        G_proj = ox.project_graph(G)\n        \n        # 2. Buildings\n        print(f\"  \u2192 Buildings...\")\n        buildings = ox.features_from_point((lat, lon), dist=RADIUS, tags={'building': True})\n        buildings_proj = buildings.to_crs(ox.graph_to_gdfs(G_proj, nodes=False).crs)\n        buildings_proj = buildings_proj[buildings_proj.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n        \n        # Convert MultiPolygons to Polygons\n        def get_polygon(geom):\n            if geom.geom_type == 'Polygon':\n                return geom\n            elif geom.geom_type == 'MultiPolygon':\n                return max(geom.geoms, key=lambda p: p.area)\n            return geom\n        \n        buildings_proj['geometry'] = buildings_proj.geometry.apply(get_polygon)\n        buildings_proj = buildings_proj[buildings_proj.geometry.type == 'Polygon'].copy()\n        \n        # 3. Building Parcels - TRY MULTIPLE LAYERS for complete coverage\n        print(f\"  \u2192 Building parcels (multiple layers)...\")\n        parcels_list = []\n        \n        # Try landuse tags\n        try:\n            parcels_landuse = ox.features_from_point((lat, lon), dist=RADIUS, tags={'landuse': True})\n            parcels_list.append(parcels_landuse)\n            print(f\"    \u2713 Found {len(parcels_landuse)} landuse parcels\")\n        except:\n            pass\n        \n        # Try boundary tags (administrative boundaries, property boundaries)\n        try:\n            parcels_boundary = ox.features_from_point((lat, lon), dist=RADIUS, tags={'boundary': 'administrative'})\n            parcels_list.append(parcels_boundary)\n            print(f\"    \u2713 Found {len(parcels_boundary)} boundary parcels\")\n        except:\n            pass\n        \n        # Combine all parcel sources\n        if parcels_list:\n            parcels = pd.concat(parcels_list, ignore_index=True)\n            parcels = gpd.GeoDataFrame(parcels, geometry='geometry')\n            parcels_proj = parcels.to_crs(ox.graph_to_gdfs(G_proj, nodes=False).crs)\n            parcels_proj = parcels_proj[parcels_proj.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n            parcels_proj['geometry'] = parcels_proj.geometry.apply(get_polygon)\n            parcels_proj = parcels_proj[parcels_proj.geometry.type == 'Polygon'].copy()\n            # Remove duplicates\n            parcels_proj = parcels_proj.drop_duplicates(subset=['geometry'])\n            print(f\"    \u2713 Total {len(parcels_proj)} parcels (combined)\")\n        else:\n            print(f\"    \u26a0 No parcels found\")\n            parcels_proj = gpd.GeoDataFrame(columns=['geometry'], crs=ox.graph_to_gdfs(G_proj, nodes=False).crs)\n        \n        # Store data\n        city_data[city_key] = {\n            'name': city_info['name'],\n            'color': city_info['color'],\n            'coords': (lat, lon),\n            'graph': G_proj,\n            'buildings': buildings_proj,\n            'parcels': parcels_proj,\n            'crs': ox.graph_to_gdfs(G_proj, nodes=False).crs\n        }\n        \n        print(f\"  \u2713 Downloaded:\")\n        print(f\"    - {G_proj.number_of_nodes()} nodes\")\n        print(f\"    - {G_proj.number_of_edges()} edges\")\n        print(f\"    - {len(buildings_proj)} buildings\")\n        print(f\"    - {len(parcels_proj)} parcels\")\n        \n    except Exception as e:\n        print(f\"  \u2717 Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\nprint(f\"\\n{'='*60}\")\nprint(f\"\u2713 Data acquisition complete for {len(city_data)} cities\")\nprint(f\"{'='*60}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 NEW: Base Map Visualization\n",
    "Display the full urban context before analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Base maps showing roads + buildings + parcels\nprint(\"\\n\" + \"=\"*60)\nprint(\"Creating base maps...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    _, edges = ox.graph_to_gdfs(city_data[city_key]['graph'])\n    buildings = city_data[city_key]['buildings']\n    parcels = city_data[city_key]['parcels']\n    \n    # Plot parcels in light green (if available)\n    if len(parcels) > 0:\n        parcels.plot(ax=ax, color='#E8F5E9', edgecolor='#66BB6A', linewidth=0.5, alpha=0.3)\n    \n    # Plot buildings in gray\n    buildings.plot(ax=ax, color='#BDBDBD', edgecolor='#424242', linewidth=0.3, alpha=0.8)\n    \n    # Plot roads in black\n    edges.plot(ax=ax, color='#000000', linewidth=1.5, alpha=0.9)\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"{len(buildings)} buildings \u00b7 {len(edges)} roads \u00b7 {len(parcels)} parcels\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n\nplt.suptitle('Base Maps: Urban Context (500\u00d7500m)', fontsize=22, fontweight='bold', y=1.0)\nplt.tight_layout()\n\n# Save both formats\nplt.savefig(VIZ_PNG_DIR / '00_base_maps.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '00_base_maps.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 00_base_maps.png + 00_base_maps.svg\")\nprint(\"  (Buildings in gray, roads in black, parcels in light green)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node Analysis (Centrality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_metrics(G):\n",
    "    \"\"\"Compute centrality metrics for nodes\"\"\"\n",
    "    print(\"  Computing node centrality...\")\n",
    "    \n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    print(\"    - Betweenness (distance)...\")\n",
    "    bc_dist = nx.betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    bc_info = nx.betweenness_centrality(G_undir, weight=None, normalized=True)\n",
    "    \n",
    "    # Closeness centrality\n",
    "    print(\"    - Closeness...\")\n",
    "    closeness = nx.closeness_centrality(G_undir, distance='length')\n",
    "    \n",
    "    # Reach centrality\n",
    "    print(\"    - Reach (200m, 300m)...\")\n",
    "    reach_200 = {}\n",
    "    reach_300 = {}\n",
    "    for node in G_undir.nodes():\n",
    "        reach_200[node] = len(nx.single_source_dijkstra_path_length(G_undir, node, cutoff=200, weight='length'))\n",
    "        reach_300[node] = len(nx.single_source_dijkstra_path_length(G_undir, node, cutoff=300, weight='length'))\n",
    "    \n",
    "    degree = dict(G_undir.degree())\n",
    "    \n",
    "    nodes, _ = ox.graph_to_gdfs(G)\n",
    "    nodes['bc_distance'] = nodes.index.map(bc_dist)\n",
    "    nodes['bc_information'] = nodes.index.map(bc_info)\n",
    "    nodes['closeness'] = nodes.index.map(closeness)\n",
    "    nodes['reach_200m'] = nodes.index.map(reach_200)\n",
    "    nodes['reach_300m'] = nodes.index.map(reach_300)\n",
    "    nodes['degree'] = nodes.index.map(degree)\n",
    "    \n",
    "    print(\"  \u2713 Node metrics computed\")\n",
    "    return nodes\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    city_data[city_key]['nodes'] = compute_node_metrics(city_data[city_key]['graph'])\n",
    "    city_data[city_key]['nodes'].to_file(GEOJSON_DIR / f\"{city_key}_nodes.geojson\", driver='GeoJSON')\n",
    "    print(f\"  \u2713 Saved to {city_key}_nodes.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 1. NODES - Centrality-based visualization\nprint(\"\\n\" + \"=\"*60)\nprint(\"1/6: Creating Node Centrality visualizations...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    nodes_gdf = city_data[city_key]['nodes']\n    edges_gdf = city_data[city_key]['edges']\n    \n    # Plot edges first (background)\n    edges_gdf.plot(ax=ax, color='#CCCCCC', linewidth=0.8, alpha=0.5)\n    \n    # Plot nodes with size based on betweenness centrality\n    node_sizes = (nodes_gdf['bc_distance'] * 500) + 10  # Scale for visibility\n    nodes_gdf.plot(\n        ax=ax,\n        markersize=node_sizes,\n        color='#FF4444',\n        alpha=0.7,\n        edgecolor='darkred',\n        linewidth=0.5\n    )\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"Nodes sized by betweenness centrality\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('1. NODES: Intersection Centrality', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '01_nodes_centrality.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '01_nodes_centrality.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 01_nodes_centrality (PNG + SVG)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Edge Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_metrics(G):\n",
    "    \"\"\"Compute edge metrics\"\"\"\n",
    "    print(\"  Computing edge metrics...\")\n",
    "    \n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    print(\"    - Edge betweenness...\")\n",
    "    edge_bc = nx.edge_betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    \n",
    "    # Dual graph for angular analysis\n",
    "    print(\"    - Angular betweenness...\")\n",
    "    dual_G = nx.Graph()\n",
    "    edge_to_node = {}\n",
    "    for i, (u, v, k) in enumerate(G_undir.edges(keys=True)):\n",
    "        edge_to_node[(u, v, k)] = i\n",
    "        dual_G.add_node(i, primal_edge=(u, v, k))\n",
    "    \n",
    "    for node in G_undir.nodes():\n",
    "        incident_edges = list(G_undir.edges(node, keys=True))\n",
    "        for i in range(len(incident_edges)):\n",
    "            for j in range(i+1, len(incident_edges)):\n",
    "                e1, e2 = incident_edges[i], incident_edges[j]\n",
    "                e1_norm = tuple(sorted([e1[0], e1[1]])) + (e1[2],)\n",
    "                e2_norm = tuple(sorted([e2[0], e2[1]])) + (e2[2],)\n",
    "                if e1_norm in edge_to_node and e2_norm in edge_to_node:\n",
    "                    dual_G.add_edge(edge_to_node[e1_norm], edge_to_node[e2_norm])\n",
    "    \n",
    "    dual_bc = nx.betweenness_centrality(dual_G, weight=None, normalized=True) if dual_G.number_of_edges() > 0 else {}\n",
    "    angular_bc = {}\n",
    "    for dual_node, bc_val in dual_bc.items():\n",
    "        primal_edge = dual_G.nodes[dual_node].get('primal_edge')\n",
    "        if primal_edge:\n",
    "            angular_bc[primal_edge] = bc_val\n",
    "    \n",
    "    _, edges = ox.graph_to_gdfs(G)\n",
    "    edges['edge_bc'] = edges.index.map(lambda x: edge_bc.get((x[0], x[1]), 0))\n",
    "    edges['angular_bc'] = edges.index.map(lambda x: angular_bc.get(x, 0))\n",
    "    \n",
    "    print(\"  \u2713 Edge metrics computed\")\n",
    "    return edges\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    city_data[city_key]['edges'] = compute_edge_metrics(city_data[city_key]['graph'])\n",
    "    city_data[city_key]['edges'].to_file(GEOJSON_DIR / f\"{city_key}_edges.geojson\", driver='GeoJSON')\n",
    "    print(f\"  \u2713 Saved to {city_key}_edges.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 4. PATHS - Edge betweenness (movement corridors)\nprint(\"\\n\" + \"=\"*60)\nprint(\"4/6: Creating Path (edge betweenness) visualizations...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    edges_gdf = city_data[city_key]['edges']\n    \n    # Plot edges with width based on betweenness\n    max_bc = edges_gdf['edge_bc'].max()\n    if max_bc > 0:\n        linewidths = (edges_gdf['edge_bc'] / max_bc) * 5 + 0.5\n    else:\n        linewidths = 1.0\n    \n    edges_gdf.plot(\n        ax=ax,\n        column='edge_bc',\n        cmap='YlOrRd',\n        linewidth=linewidths,\n        alpha=0.8,\n        legend=False\n    )\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"Thickness = betweenness centrality\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('4. PATHS: Movement Corridors (Edge Betweenness)', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '04_paths_betweenness.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '04_paths_betweenness.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 04_paths_betweenness (PNG + SVG)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 5. EDGES - Street connectivity by length\nprint(\"\\n\" + \"=\"*60)\nprint(\"5/6: Creating Edge (connectivity) visualizations...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 3, figsize=(24, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    edges_gdf = city_data[city_key]['edges']\n    \n    # Plot with edge length (connectivity structure)\n    edges_gdf.plot(\n        ax=ax,\n        column='length',\n        cmap='viridis',\n        linewidth=2.5,\n        alpha=0.8,\n        legend=False\n    )\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"Color = street segment length\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('5. EDGES: Street Network Connectivity', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '05_edges_connectivity.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '05_edges_connectivity.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 05_edges_connectivity (PNG + SVG)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parcel Analysis\n",
    "Process building parcels (landuse boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process parcels and compute metrics\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    parcels = city_data[city_key]['parcels']\n",
    "    \n",
    "    if len(parcels) > 0:\n",
    "        print(\"  Processing parcels...\")\n",
    "        parcels['area'] = parcels.geometry.area\n",
    "        parcels['perimeter'] = parcels.geometry.length\n",
    "        parcels['compactness'] = (4 * np.pi * parcels['area']) / (parcels['perimeter'] ** 2)\n",
    "        \n",
    "        # Filter by size\n",
    "        parcels_filtered = parcels[\n",
    "            (parcels['area'] >= MIN_PARCEL_AREA) & \n",
    "            (parcels['area'] <= MAX_PARCEL_AREA)\n",
    "        ].copy()\n",
    "        \n",
    "        # Compute aspect ratio\n",
    "        aspect_ratios = []\n",
    "        for geom in parcels_filtered.geometry:\n",
    "            try:\n",
    "                mbr = geom.minimum_rotated_rectangle\n",
    "                coords = list(mbr.exterior.coords)\n",
    "                side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "                side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "                aspect = max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0\n",
    "                aspect_ratios.append(aspect)\n",
    "            except:\n",
    "                aspect_ratios.append(1.0)\n",
    "        \n",
    "        parcels_filtered['aspect_ratio'] = aspect_ratios\n",
    "        parcels_filtered['parcel_id'] = [f\"parcel_{i:03d}\" for i in range(len(parcels_filtered))]\n",
    "        \n",
    "        city_data[city_key]['parcels_processed'] = parcels_filtered\n",
    "        \n",
    "        # Save\n",
    "        parcels_filtered.to_file(GEOJSON_DIR / f\"{city_key}_parcels.geojson\", driver='GeoJSON')\n",
    "        print(f\"  \u2713 Processed {len(parcels_filtered)} parcels (filtered from {len(parcels)})\")\n",
    "        print(f\"  \u2713 Saved to {city_key}_parcels.geojson\")\n",
    "    else:\n",
    "        print(\"  \u26a0 No parcels available\")\n",
    "        city_data[city_key]['parcels_processed'] = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 3. PARCELS - Building parcel visualization\nprint(\"\\n\" + \"=\"*60)\nprint(\"3/6: Creating Parcel visualizations...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    parcels = city_data[city_key]['parcels_processed']\n    edges_gdf = city_data[city_key]['edges']\n    \n    # Plot edges (background)\n    edges_gdf.plot(ax=ax, color='#CCCCCC', linewidth=0.5, alpha=0.3)\n    \n    if len(parcels) > 0:\n        # Plot parcels in uniform color (no district coloring)\n        parcels.plot(\n            ax=ax,\n            color='#B3E5FC',\n            edgecolor='#0277BD',\n            linewidth=1.2,\n            alpha=0.6\n        )\n        \n        parcel_info = f\"{len(parcels)} parcels\"\n    else:\n        parcel_info = \"No parcels\"\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"{parcel_info}\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('3. PARCELS: Building Parcel Boundaries', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '03_parcels.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '03_parcels.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 03_parcels (PNG + SVG)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. District Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NetworkX has built-in Louvain community detection (since v2.5+)\n# No external packages needed\nprint(\"\u2713 Using NetworkX built-in Louvain community detection\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def detect_districts(G, method='distance'):\n    \"\"\"Detect districts using community detection\"\"\"\n    print(f\"    - {method}...\")\n    try:\n        G_undir = G.to_undirected()\n        G_simple = nx.Graph()\n        for u, v, data in G_undir.edges(data=True):\n            if not G_simple.has_edge(u, v):\n                G_simple.add_edge(u, v, **data)\n        \n        # Use NetworkX built-in Louvain\n        if method == 'distance':\n            communities = nx.algorithms.community.louvain_communities(G_simple, weight='length')\n        else:\n            communities = nx.algorithms.community.louvain_communities(G_simple, weight=None)\n        \n        # Convert from list of sets to node->community_id dict\n        partition = {}\n        for comm_id, community in enumerate(communities):\n            for node in community:\n                partition[node] = comm_id\n        \n        return partition\n    except Exception as e:\n        print(f\"      \u2717 Error: {e}\")\n        return {node: 0 for node in G.nodes()}\n\nfor city_key in city_data.keys():\n    print(f\"\\n{city_data[city_key]['name']}:\")\n    G = city_data[city_key]['graph']\n    nodes = city_data[city_key]['nodes']\n    \n    partitions = {}\n    for method in ['distance', 'angular', 'topological']:\n        partition = detect_districts(G, method=method)\n        partitions[method] = partition\n        \n        nodes_districts = nodes.copy()\n        nodes_districts['district'] = nodes_districts.index.map(partition)\n        nodes_districts.to_file(GEOJSON_DIR / f\"{city_key}_districts_{method}.geojson\", driver='GeoJSON')\n        \n        print(f\"      {method}: {len(set(partition.values()))} districts\")\n    \n    city_data[city_key]['partitions'] = partitions\n    print(f\"  \u2713 District detection complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Landmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize(series, default=0.5):\n",
    "    \"\"\"Safely normalize, handling NaN and min==max\"\"\"\n",
    "    min_val, max_val = series.min(), series.max()\n",
    "    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:\n",
    "        return pd.Series([default] * len(series), index=series.index)\n",
    "    return ((series - min_val) / (max_val - min_val)).fillna(default)\n",
    "\n",
    "def compute_landmark_scores(buildings_gdf, edges_gdf):\n",
    "    \"\"\"Compute landmark scores\"\"\"\n",
    "    print(\"  Computing landmark scores...\")\n",
    "    buildings = buildings_gdf.copy()\n",
    "    \n",
    "    # Structural score\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    buildings['s_area'] = safe_normalize(buildings['area'])\n",
    "    \n",
    "    street_union = unary_union(edges_gdf.geometry)\n",
    "    buildings['dist_to_street'] = buildings.geometry.apply(lambda g: g.distance(street_union))\n",
    "    max_dist = buildings['dist_to_street'].max()\n",
    "    buildings['s_visibility'] = (1 - buildings['dist_to_street'] / max_dist) if max_dist > 0 else 0.5\n",
    "    buildings['s_visibility'] = buildings['s_visibility'].fillna(0.5)\n",
    "    buildings['structural_score'] = (0.6 * buildings['s_area'] + 0.4 * buildings['s_visibility']).fillna(0.5)\n",
    "    \n",
    "    # Other scores\n",
    "    buildings['visual_score'] = 0.5\n",
    "    buildings['cultural_score'] = 0.0\n",
    "    buildings['pragmatic_score'] = 0.0\n",
    "    buildings['global_score'] = (0.4 * buildings['structural_score'] + 0.2 * buildings['visual_score'] + \n",
    "                                   0.2 * buildings['cultural_score'] + 0.2 * buildings['pragmatic_score']).fillna(0.5)\n",
    "    \n",
    "    # Geometry metrics\n",
    "    aspect_ratios = []\n",
    "    for geom in buildings.geometry:\n",
    "        try:\n",
    "            mbr = geom.minimum_rotated_rectangle\n",
    "            coords = list(mbr.exterior.coords)\n",
    "            side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "            side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "            aspect_ratios.append(max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0)\n",
    "        except:\n",
    "            aspect_ratios.append(1.0)\n",
    "    buildings['aspect_ratio'] = aspect_ratios\n",
    "    buildings['setback_dist'] = buildings['dist_to_street']\n",
    "    \n",
    "    print(\"  \u2713 Landmark scores computed\")\n",
    "    return buildings\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    buildings_scored = compute_landmark_scores(city_data[city_key]['buildings'], city_data[city_key]['edges'])\n",
    "    city_data[city_key]['buildings_scored'] = buildings_scored\n",
    "    \n",
    "    cols = ['geometry', 'area', 'structural_score', 'global_score', 'aspect_ratio', 'setback_dist']\n",
    "    buildings_scored[cols].to_file(GEOJSON_DIR / f\"{city_key}_buildings.geojson\", driver='GeoJSON')\n",
    "    print(f\"  \u2713 Saved to {city_key}_buildings.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2. LANDMARKS - Color-coded by importance score\nprint(\"\\n\" + \"=\"*60)\nprint(\"2/6: Creating Landmark visualizations...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    buildings = city_data[city_key]['buildings_scored']\n    edges_gdf = city_data[city_key]['edges']\n    \n    # Plot edges (background)\n    edges_gdf.plot(ax=ax, color='#EEEEEE', linewidth=0.5, alpha=0.3)\n    \n    # Identify landmarks: use ONLY hard threshold (no max limit)\n    landmarks = buildings[buildings['global_score'] >= LANDMARK_THRESHOLD].copy()\n    regular = buildings[buildings['global_score'] < LANDMARK_THRESHOLD].copy()\n    \n    # Plot regular buildings in light gray\n    if len(regular) > 0:\n        regular.plot(ax=ax, color='#E0E0E0', edgecolor='#999999', linewidth=0.2, alpha=0.5)\n    \n    # Plot landmarks in red (graduated by score)\n    if len(landmarks) > 0:\n        landmarks.plot(\n            ax=ax,\n            column='global_score',\n            cmap='Reds',\n            edgecolor='darkred',\n            linewidth=0.5,\n            alpha=0.9,\n            vmin=LANDMARK_THRESHOLD,\n            vmax=1.0\n        )\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"{len(landmarks)} landmarks (score \u2265 {LANDMARK_THRESHOLD:.1f})\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('2. LANDMARKS: Building Importance', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '02_landmarks.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '02_landmarks.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 02_landmarks (PNG + SVG)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building Footprint Library\n",
    "Extract diverse individual building footprints (not blocks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_building_footprint_library(buildings_gdf, city_key, target_count=35):\n",
    "    \"\"\"Extract diverse building footprints\"\"\"\n",
    "    print(f\"  Extracting {target_count} footprints...\")\n",
    "    \n",
    "    if len(buildings_gdf) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by area for diversity\n",
    "    buildings = buildings_gdf.copy().sort_values('area')\n",
    "    \n",
    "    if len(buildings) <= target_count:\n",
    "        selected = buildings\n",
    "    else:\n",
    "        indices = np.linspace(0, len(buildings)-1, target_count, dtype=int)\n",
    "        selected = buildings.iloc[indices]\n",
    "    \n",
    "    library = []\n",
    "    for idx, (_, row) in enumerate(selected.iterrows()):\n",
    "        geom = row.geometry\n",
    "        centroid = geom.centroid\n",
    "        \n",
    "        # Translate to origin\n",
    "        translated = translate(geom, xoff=-centroid.x, yoff=-centroid.y)\n",
    "        \n",
    "        library.append({\n",
    "            'footprint_id': f\"{city_key}_building_{idx:03d}\",\n",
    "            'city': city_key,\n",
    "            'area': float(row['area']),\n",
    "            'aspect_ratio': float(row.get('aspect_ratio', 1.0)),\n",
    "            'structural_score': float(row.get('structural_score', 0.5)),\n",
    "            'geometry': {\n",
    "                'type': 'Polygon',\n",
    "                'coordinates': [list(translated.exterior.coords)]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    print(f\"  \u2713 Extracted {len(library)} footprints\")\n",
    "    return library\n",
    "\n",
    "# Extract for all cities\n",
    "all_footprints = []\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    footprints = extract_building_footprint_library(\n",
    "        city_data[city_key]['buildings_scored'],\n",
    "        city_key,\n",
    "        FOOTPRINTS_PER_CITY\n",
    "    )\n",
    "    all_footprints.extend(footprints)\n",
    "    city_data[city_key]['footprint_library'] = footprints\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\u2713 Total footprint library: {len(all_footprints)} buildings\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save\n",
    "if len(all_footprints) > 0:\n",
    "    with open(METRICS_DIR / 'building_footprint_library.json', 'w') as f:\n",
    "        json.dump(all_footprints, f, indent=2)\n",
    "    print(f\"\u2713 Saved to building_footprint_library.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution(values, bins=20):\n",
    "    if len(values) == 0:\n",
    "        return {'bins': [], 'counts': [], 'mean': 0, 'median': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "    hist, bin_edges = np.histogram(values, bins=bins)\n",
    "    return {\n",
    "        'bins': bin_edges.tolist(),\n",
    "        'counts': hist.tolist(),\n",
    "        'mean': float(np.mean(values)),\n",
    "        'median': float(np.median(values)),\n",
    "        'std': float(np.std(values)),\n",
    "        'min': float(np.min(values)),\n",
    "        'max': float(np.max(values))\n",
    "    }\n",
    "\n",
    "urban_metrics = {}\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\nAggregating: {city_data[city_key]['name']}...\")\n",
    "    \n",
    "    nodes = city_data[city_key]['nodes']\n",
    "    edges = city_data[city_key]['edges']\n",
    "    parcels = city_data[city_key]['parcels_processed']\n",
    "    buildings = city_data[city_key]['buildings_scored']\n",
    "    partitions = city_data[city_key]['partitions']\n",
    "    \n",
    "    urban_metrics[city_key] = {\n",
    "        'name': city_data[city_key]['name'],\n",
    "        'nodes': {\n",
    "            'total_count': len(nodes),\n",
    "            'avg_degree': float(nodes['degree'].mean()),\n",
    "            'degree_distribution': nodes['degree'].value_counts().to_dict()\n",
    "        },\n",
    "        'edges': {\n",
    "            'total_count': len(edges),\n",
    "            'total_length_m': float(edges['length'].sum()),\n",
    "            'density_m_per_km2': float(edges['length'].sum() / 0.25),\n",
    "            'segment_length_distribution': compute_distribution(edges['length'].values)\n",
    "        },\n",
    "        'parcels': {\n",
    "            'total_count': len(parcels),\n",
    "            'area_distribution': compute_distribution(parcels['area'].values) if len(parcels) > 0 else {}\n",
    "        },\n",
    "        'buildings': {\n",
    "            'total_count': len(buildings),\n",
    "            'area_distribution': compute_distribution(buildings['area'].values),\n",
    "            'aspect_ratio_distribution': compute_distribution(buildings['aspect_ratio'].values)\n",
    "        },\n",
    "        'districts': {\n",
    "            'count_distance': len(set(partitions['distance'].values())),\n",
    "            'count_angular': len(set(partitions['angular'].values())),\n",
    "            'count_topological': len(set(partitions['topological'].values()))\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(METRICS_DIR / 'urban_metrics.json', 'w') as f:\n",
    "    json.dump({'urban_metrics': urban_metrics}, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\u2713 Saved to urban_metrics.json\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metrics Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\ud83d\udcca METRICS SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "table_data = []\n",
    "metrics = [\n",
    "    ('Nodes', lambda m: m['nodes']['total_count']),\n",
    "    ('Edges', lambda m: m['edges']['total_count']),\n",
    "    ('Street Length (m)', lambda m: m['edges']['total_length_m']),\n",
    "    ('Street Density (m/km\u00b2)', lambda m: m['edges']['density_m_per_km2']),\n",
    "    ('Avg Segment (m)', lambda m: m['edges']['segment_length_distribution']['mean']),\n",
    "    ('Parcels', lambda m: m['parcels']['total_count']),\n",
    "    ('Buildings', lambda m: m['buildings']['total_count']),\n",
    "    ('Avg Building Area (m\u00b2)', lambda m: m['buildings']['area_distribution']['mean']),\n",
    "    ('Districts (distance)', lambda m: m['districts']['count_distance'])\n",
    "]\n",
    "\n",
    "for metric_name, func in metrics:\n",
    "    row = {'Metric': metric_name}\n",
    "    for city_key in city_data.keys():\n",
    "        try:\n",
    "            row[city_data[city_key]['name']] = f\"{func(urban_metrics[city_key]):.1f}\"\n",
    "        except:\n",
    "            row[city_data[city_key]['name']] = 'N/A'\n",
    "    table_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(table_data).set_index('Metric')\n",
    "print(\"\\n\" + df.to_string())\n",
    "\n",
    "df.to_csv(METRICS_DIR / 'metrics_summary.csv')\n",
    "print(f\"\\n\u2713 Saved to metrics_summary.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# RADAR METRICS COMPUTATION - Must come BEFORE visualizations\nprint(\"\\n\" + \"=\"*60)\nprint(\"Computing radar chart metrics...\")\nprint(\"=\"*60)\n\n# Compute meaningful metrics for each city\nradar_data = {}\n\nfor city_key in city_data.keys():\n    nodes = city_data[city_key]['nodes']\n    edges = city_data[city_key]['edges']\n    buildings = city_data[city_key]['buildings_scored']\n    G = city_data[city_key]['graph']\n    partition = city_data[city_key]['partitions']['distance']\n    \n    # 1. Node Integration: Mean betweenness centrality\n    node_integration = float(nodes['bc_distance'].mean())\n    \n    # 2. Edge Connectivity: Average closeness centrality\n    edge_connectivity = float(nodes['closeness'].mean())\n    \n    # 3. Landmark Prominence: Mean score of landmarks (using hard threshold 0.4)\n    landmarks = buildings[buildings['global_score'] >= LANDMARK_THRESHOLD]\n    if len(landmarks) > 0:\n        landmark_prominence = float(landmarks['global_score'].mean())\n    else:\n        landmark_prominence = 0.0\n    \n    # 4. District Coherence: Inverse of district density (fewer districts = more coherent)\n    n_districts = len(set(partition.values()))\n    n_nodes = len(nodes)\n    district_coherence = 1 - (n_districts / n_nodes) if n_nodes > 0 else 0.5\n    \n    # 5. Path Efficiency: Ratio of high-betweenness edges (concentrated flow = higher efficiency)\n    # More concentrated movement (fewer but stronger paths) = higher efficiency\n    if len(edges) > 0:\n        # Calculate what percentage of edges carry significant flow\n        edge_bc_sorted = edges['edge_bc'].sort_values(ascending=False)\n        top_20pct_idx = max(1, int(len(edge_bc_sorted) * 0.2))\n        top_20pct_flow = edge_bc_sorted.iloc[:top_20pct_idx].sum()\n        # Higher value = more concentrated/efficient path structure\n        path_efficiency = float(top_20pct_flow)\n    else:\n        path_efficiency = 0.0\n    \n    radar_data[city_key] = {\n        'Node Integration': node_integration,\n        'Edge Connectivity': edge_connectivity,\n        'Landmark Prominence': landmark_prominence,\n        'District Coherence': district_coherence,\n        'Path Efficiency': path_efficiency\n    }\n\n# Normalize all metrics to 0-1 scale for fair comparison\nmetrics_names = ['Node Integration', 'Edge Connectivity', 'Landmark Prominence', \n                 'District Coherence', 'Path Efficiency']\n\n# Get max/min values for normalization\nmax_values = {m: max(radar_data[c][m] for c in city_data.keys()) for m in metrics_names}\nmin_values = {m: min(radar_data[c][m] for c in city_data.keys()) for m in metrics_names}\n\n# Normalize to 0-1 range\nfor city_key in radar_data.keys():\n    for metric in metrics_names:\n        val_range = max_values[metric] - min_values[metric]\n        if val_range > 0:\n            radar_data[city_key][metric] = (radar_data[city_key][metric] - min_values[metric]) / val_range\n        else:\n            radar_data[city_key][metric] = 0.5\n\nprint(\"Radar metrics computed:\")\nfor city_key in city_data.keys():\n    print(f\"  {city_data[city_key]['name']}:\")\n    for metric in metrics_names:\n        print(f\"    {metric}: {radar_data[city_key][metric]:.3f}\")\n\nprint(\"\u2713 Metrics ready for visualization\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Space Syntax Visualizations\n\nCreate comprehensive visualizations for the 5 Kevin Lynch elements:\n1. Nodes (centrality-based sizing)\n2. Landmarks (color-coded by importance)\n3. Districts (community detection)\n4. Paths (edge betweenness)\n5. Edges (connectivity)\n\nThen create overlay and analysis visualizations."
  },
  {
   "cell_type": "code",
   "source": "# BUILDING FOOTPRINTS - 6 per city, all black\nprint(\"\\n\" + \"=\"*60)\nprint(\"Creating building footprint visualization (6 per city)...\")\nprint(\"=\"*60)\n\nif len(all_footprints) > 0:\n    # Select 6 footprints per city (24 total for 4 cities)\n    selected_footprints = []\n    for city_key in city_data.keys():\n        city_footprints = city_data[city_key]['footprint_library']\n        if len(city_footprints) >= 6:\n            # Select evenly distributed footprints\n            indices = [0, len(city_footprints)//5, 2*len(city_footprints)//5, \n                      3*len(city_footprints)//5, 4*len(city_footprints)//5, len(city_footprints)-1]\n            selected = [city_footprints[i] for i in indices]\n        else:\n            selected = city_footprints[:6]\n        selected_footprints.extend(selected)\n    \n    # Create 4x6 grid\n    fig, axes = plt.subplots(4, 6, figsize=(18, 12), facecolor='white')\n    \n    for idx, fp in enumerate(selected_footprints):\n        row = idx // 6\n        col = idx % 6\n        ax = axes[row, col]\n        \n        poly = Polygon(fp['geometry']['coordinates'][0])\n        x, y = poly.exterior.xy\n        \n        # ALL BLACK\n        ax.fill(x, y, color='black', alpha=1.0, edgecolor='black', linewidth=1)\n        ax.set_title(\n            f\"{fp['city'].upper()}\\n{fp['area']:.0f}m\u00b2\",\n            fontsize=8, pad=3\n        )\n        ax.set_aspect('equal')\n        ax.axis('off')\n    \n    plt.suptitle(\n        f'Building Footprint Library (6 per city, {len(selected_footprints)} total)',\n        fontsize=16, fontweight='bold', y=0.995\n    )\n    plt.tight_layout()\n    plt.savefig(VIZ_PNG_DIR / '07_footprints_6per.png', dpi=300, bbox_inches='tight', facecolor='white')\n    plt.savefig(VIZ_SVG_DIR / '07_footprints_6per.svg', bbox_inches='tight', facecolor='white')\n    plt.show()\n    \n    print(f\"\u2713 Saved: 07_footprints_6per (PNG + SVG)\")\n    print(f\"  Grid: 4 cities \u00d7 6 footprints = {len(selected_footprints)} total\")\nelse:\n    print(\"\u26a0 No footprints to visualize\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 9. RADAR CHARTS - Individual city visualizations\nprint(\"\\nCreating individual radar charts...\")\n\nfig, axes = plt.subplots(1, 4, figsize=(28, 7), subplot_kw=dict(projection='polar'), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    # Get values\n    values = [radar_data[city_key][m] for m in metrics_names]\n    values += values[:1]  # Close the polygon\n    \n    # Set up angles\n    angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\n    angles += angles[:1]\n    \n    # Plot\n    ax.plot(angles, values, 'o-', linewidth=2.5, color=city_data[city_key]['color'], \n            markersize=8, alpha=0.8)\n    ax.fill(angles, values, alpha=0.25, color=city_data[city_key]['color'])\n    \n    # Formatting\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(metrics_names, fontsize=10, fontweight='bold')\n    ax.set_ylim(0, 1)\n    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=8)\n    ax.grid(True, alpha=0.3)\n    ax.set_title(f\"{city_data[city_key]['name']}\", fontsize=14, fontweight='bold', pad=20)\n\nplt.suptitle('Space Syntax Metrics: Individual Cities', fontsize=20, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '08_radar_individual.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '08_radar_individual.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 08_radar_individual (PNG + SVG)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 10. RADAR CHART - Overlay comparison\nprint(\"\\nCreating overlaid radar chart...\")\n\nfig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'), facecolor='white')\n\n# Set up angles\nangles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()\nangles += angles[:1]\n\n# Plot all cities on same chart\nfor city_key in city_data.keys():\n    values = [radar_data[city_key][m] for m in metrics_names]\n    values += values[:1]\n    \n    ax.plot(angles, values, 'o-', linewidth=3, color=city_data[city_key]['color'],\n            label=city_data[city_key]['name'], markersize=10, alpha=0.9)\n    ax.fill(angles, values, alpha=0.15, color=city_data[city_key]['color'])\n\n# Formatting\nax.set_xticks(angles[:-1])\nax.set_xticklabels(metrics_names, fontsize=12, fontweight='bold')\nax.set_ylim(0, 1)\nax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\nax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=10)\nax.grid(True, alpha=0.3, linewidth=1.5)\nax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=12, frameon=True, shadow=True)\n\nplt.title('Space Syntax Metrics: Comparative Overlay', fontsize=18, fontweight='bold', pad=30)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '09_radar_overlay.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '09_radar_overlay.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 09_radar_overlay (PNG + SVG)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 6. OVERLAY - All 5 Kevin Lynch elements combined\nprint(\"\\n\" + \"=\"*60)\nprint(\"6/6: Creating OVERLAY visualization (all elements)...\")\nprint(\"=\"*60)\n\nfig, axes = plt.subplots(1, 4, figsize=(32, 8), facecolor='white')\n\nfor idx, city_key in enumerate(city_data.keys()):\n    ax = axes[idx]\n    \n    nodes_gdf = city_data[city_key]['nodes']\n    edges_gdf = city_data[city_key]['edges']\n    buildings = city_data[city_key]['buildings_scored']\n    partition = city_data[city_key]['partitions']['distance']\n    \n    # Layer 1: Districts (as background color on edges)\n    edge_districts = []\n    for u, v, k in edges_gdf.index:\n        d1 = partition.get(u, 0)\n        d2 = partition.get(v, 0)\n        edge_districts.append(max(d1, d2))\n    edges_with_district = edges_gdf.copy()\n    edges_with_district['district'] = edge_districts\n    n_districts = len(set(partition.values()))\n    cmap_districts = plt.cm.get_cmap('Pastel1', n_districts)\n    edges_with_district.plot(ax=ax, column='district', cmap=cmap_districts, linewidth=3, alpha=0.3)\n    \n    # Layer 2: Paths (edge betweenness as line width)\n    max_bc = edges_gdf['edge_bc'].max()\n    if max_bc > 0:\n        linewidths = (edges_gdf['edge_bc'] / max_bc) * 4 + 0.5\n    else:\n        linewidths = 1.5\n    edges_gdf.plot(ax=ax, color='#555555', linewidth=linewidths, alpha=0.6)\n    \n    # Layer 3: Landmarks (use hard threshold only)\n    landmarks = buildings[buildings['global_score'] >= LANDMARK_THRESHOLD].copy()\n    regular = buildings[buildings['global_score'] < LANDMARK_THRESHOLD].copy()\n    \n    if len(regular) > 0:\n        regular.plot(ax=ax, color='#DDDDDD', edgecolor='#AAAAAA', linewidth=0.1, alpha=0.4)\n    if len(landmarks) > 0:\n        landmarks.plot(ax=ax, color='#FF0000', edgecolor='darkred', linewidth=0.5, alpha=0.8)\n    \n    # Layer 4: Nodes (sized by centrality)\n    node_sizes = (nodes_gdf['bc_distance'] * 300) + 15\n    nodes_gdf.plot(ax=ax, markersize=node_sizes, color='#0066CC', \n                   alpha=0.7, edgecolor='white', linewidth=1, zorder=5)\n    \n    ax.set_title(\n        f\"{city_data[city_key]['name']}\\n\"\n        f\"Districts \u2022 Paths \u2022 {len(landmarks)} Landmarks \u2022 Nodes\",\n        fontsize=14, fontweight='bold', pad=15\n    )\n    ax.set_xlabel('Easting (m)', fontsize=11)\n    ax.set_ylabel('Northing (m)', fontsize=11)\n    ax.tick_params(labelsize=9)\n    ax.grid(True, alpha=0.2)\n\nplt.suptitle('OVERLAY: Complete Image of the City (Kevin Lynch)', fontsize=22, fontweight='bold', y=0.98)\nplt.tight_layout()\nplt.savefig(VIZ_PNG_DIR / '06_overlay_all_elements.png', dpi=300, bbox_inches='tight', facecolor='white')\nplt.savefig(VIZ_SVG_DIR / '06_overlay_all_elements.svg', bbox_inches='tight', facecolor='white')\nplt.show()\n\nprint(\"\u2713 Saved: 06_overlay_all_elements (PNG + SVG)\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"\u2713 All 6 space syntax visualizations complete!\")\nprint(\"=\"*60)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"\u2713 STEP 1 COMPLETE: COMPREHENSIVE URBAN ANALYSIS\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83d\udcc1 OUTPUTS:\")\nprint(f\"  GeoJSON: {len(list(GEOJSON_DIR.glob('*.geojson')))} files\")\nprint(f\"  PNG: {len(list(VIZ_PNG_DIR.glob('*.png')))} files\")\nprint(f\"  SVG: {len(list(VIZ_SVG_DIR.glob('*.svg')))} files\")\nprint(f\"  Metrics: {len(list(METRICS_DIR.glob('*')))} files\")\n\nprint(\"\\n\ud83d\udcca VISUALIZATIONS CREATED:\")\nprint(\"  1. Base Maps (00)\")\nprint(\"  2. Node Centrality (01)\")\nprint(\"  3. Landmarks (02)\")\nprint(\"  4. Districts (03)\")\nprint(\"  5. Paths/Edge Betweenness (04)\")\nprint(\"  6. Edges/Angular (05)\")\nprint(\"  7. Complete Overlay (06)\")\nprint(\"  8. ALL Building Footprints - 105 total (07)\")\nprint(\"  9. Radar Charts - Individual (08)\")\nprint(\"  10. Radar Chart - Overlay (09)\")\nprint(\"  11. Improved Histograms (10)\")\n\nprint(\"\\n\ud83c\udfd7\ufe0f SPACE SYNTAX ELEMENTS:\")\nprint(\"  \u2713 Nodes: Sized by betweenness centrality\")\nprint(\"  \u2713 Landmarks: Color-coded by importance (top 10%)\")\nprint(\"  \u2713 Districts: Community detection with Louvain\")\nprint(\"  \u2713 Paths: Edge betweenness (movement corridors)\")\nprint(\"  \u2713 Edges: Angular integration\")\nprint(\"  \u2713 Overlay: All 5 elements combined\")\n\nprint(\"\\n\ud83d\udcd0 METRICS:\")\nfor city_key in city_data.keys():\n    m = urban_metrics[city_key]\n    print(f\"  {m['name']}:\")\n    print(f\"    - {m['buildings']['total_count']} buildings\")\n    print(f\"    - {m['parcels']['total_count']} parcels\")\n    print(f\"    - {len(city_data[city_key]['footprint_library'])} in footprint library\")\n    print(f\"    - {m['districts']['count_distance']} districts\")\n    print(f\"    - Node Integration: {radar_data[city_key]['Node Integration']:.3f}\")\n\nprint(f\"\\n  TOTAL FOOTPRINT LIBRARY: {len(all_footprints)} buildings\")\n\nprint(\"\\n\u2705 DELIVERABLES:\")\nprint(\"  \u2713 All visualizations: PNG (300 DPI) + SVG (vector)\")\nprint(\"  \u2713 Building footprints: Individual shapes (NOT blocks)\")\nprint(\"  \u2713 Building parcels: Landuse boundaries from OSM\")\nprint(\"  \u2713 Space syntax: 5 Kevin Lynch elements visualized\")\nprint(\"  \u2713 Radar charts: Meaningful metrics for comparative analysis\")\nprint(\"  \u2713 Improved histograms: Bar-based with outlier removal\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2713 STEP 1 COMPLETE: URBAN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\ud83d\udcc1 OUTPUTS:\")\n",
    "print(f\"  GeoJSON: {len(list(GEOJSON_DIR.glob('*.geojson')))} files\")\n",
    "print(f\"  PNG: {len(list(VIZ_PNG_DIR.glob('*.png')))} files\")\n",
    "print(f\"  SVG: {len(list(VIZ_SVG_DIR.glob('*.svg')))} files\")\n",
    "print(f\"  Metrics: {len(list(METRICS_DIR.glob('*')))} files\")\n",
    "\n",
    "print(\"\\n\ud83d\udcca RESULTS:\")\n",
    "for city_key in city_data.keys():\n",
    "    m = urban_metrics[city_key]\n",
    "    print(f\"  {m['name']}: {m['buildings']['total_count']} buildings, \"\n",
    "          f\"{m['parcels']['total_count']} parcels, \"\n",
    "          f\"{len(city_data[city_key]['footprint_library'])} in library\")\n",
    "\n",
    "print(f\"\\n  TOTAL FOOTPRINT LIBRARY: {len(all_footprints)} buildings\")\n",
    "\n",
    "print(\"\\n\u2713 All visualizations: PNG + SVG\")\n",
    "print(\"\u2713 Building footprints: Individual shapes (not blocks)\")\n",
    "print(\"\u2713 Parcels: Landuse boundaries from OSM\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}