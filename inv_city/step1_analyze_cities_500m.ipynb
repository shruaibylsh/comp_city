{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Analyze Real Cities (500√ó500m)\n",
    "## Extract Urban Metrics & Building Footprint Library\n",
    "\n",
    "**Goal**: Analyze three 500√ó500m urban areas to extract:\n",
    "- Space syntax metrics (nodes, edges, districts, landmarks)\n",
    "- Building footprint library (individual building shapes)\n",
    "- Building parcels (land use boundaries)\n",
    "- Building geometry distributions\n",
    "\n",
    "**Cities**:\n",
    "1. Hanoi, Vietnam (21.0230¬∞N, 105.8560¬∞E) - Dense, organic layout\n",
    "2. Brussels, Belgium (50.8477¬∞N, 4.3572¬∞E) - European historic core\n",
    "3. Marrakech, Morocco (31.623811¬∞N, -7.988662¬∞W) - Compact medina\n",
    "\n",
    "**Outputs**:\n",
    "- GeoJSON files (nodes, edges, buildings, parcels, districts)\n",
    "- JSON metrics file (urban_metrics.json)\n",
    "- Building footprint library (building_footprint_library.json)\n",
    "- Visualizations (PNG + SVG)\n",
    "- Metrics summary table (CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib import cm\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon, box\n",
    "from shapely.ops import unary_union\n",
    "from shapely.affinity import translate\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure OSMnx\n",
    "ox.settings.use_cache = True\n",
    "ox.settings.log_console = False\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CITIES = {\n",
    "    'hanoi': {\n",
    "        'name': 'Hanoi, Vietnam',\n",
    "        'coords': (21.0230, 105.8560),\n",
    "        'color': '#FF6B6B'\n",
    "    },\n",
    "    'brussels': {\n",
    "        'name': 'Brussels, Belgium',\n",
    "        'coords': (50.8477, 4.3572),\n",
    "        'color': '#4ECDC4'\n",
    "    },\n",
    "    'marrakech': {\n",
    "        'name': 'Marrakech, Morocco',\n",
    "        'coords': (31.623811, -7.988662),\n",
    "        'color': '#FFE66D'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analysis parameters (adapted for 500√ó500m)\n",
    "RADIUS = 250  # meters\n",
    "REACH_RADII = [200, 300]\n",
    "LOCAL_LANDMARK_RADIUS = 300\n",
    "MIN_PARCEL_AREA = 500  # m¬≤\n",
    "MAX_PARCEL_AREA = 10000  # m¬≤\n",
    "FOOTPRINTS_PER_CITY = 35  # Target library size\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path('outputs')\n",
    "GEOJSON_DIR = OUTPUT_DIR / 'geojson'\n",
    "VIZ_PNG_DIR = OUTPUT_DIR / 'visualizations' / 'png'\n",
    "VIZ_SVG_DIR = OUTPUT_DIR / 'visualizations' / 'svg'\n",
    "METRICS_DIR = OUTPUT_DIR / 'metrics'\n",
    "\n",
    "for d in [GEOJSON_DIR, VIZ_PNG_DIR, VIZ_SVG_DIR, METRICS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration complete\")\n",
    "print(f\"  Analyzing {len(CITIES)} cities\")\n",
    "print(f\"  Coverage radius: {RADIUS}m (~{RADIUS*2}√ó{RADIUS*2}m)\")\n",
    "print(f\"  Output: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for all cities\n",
    "city_data = {}\n",
    "\n",
    "for city_key, city_info in CITIES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Downloading: {city_info['name']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    lat, lon = city_info['coords']\n",
    "    \n",
    "    try:\n",
    "        # 1. Street network\n",
    "        print(f\"  ‚Üí Street network...\")\n",
    "        G = ox.graph_from_point((lat, lon), dist=RADIUS, network_type='walk', simplify=True)\n",
    "        G_proj = ox.project_graph(G)\n",
    "        \n",
    "        # 2. Buildings\n",
    "        print(f\"  ‚Üí Buildings...\")\n",
    "        buildings = ox.features_from_point((lat, lon), dist=RADIUS, tags={'building': True})\n",
    "        buildings_proj = buildings.to_crs(ox.graph_to_gdfs(G_proj, nodes=False).crs)\n",
    "        buildings_proj = buildings_proj[buildings_proj.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n",
    "        \n",
    "        # Convert MultiPolygons to Polygons\n",
    "        def get_polygon(geom):\n",
    "            if geom.geom_type == 'Polygon':\n",
    "                return geom\n",
    "            elif geom.geom_type == 'MultiPolygon':\n",
    "                return max(geom.geoms, key=lambda p: p.area)\n",
    "            return geom\n",
    "        \n",
    "        buildings_proj['geometry'] = buildings_proj.geometry.apply(get_polygon)\n",
    "        buildings_proj = buildings_proj[buildings_proj.geometry.type == 'Polygon'].copy()\n",
    "        \n",
    "        # 3. Building Parcels (landuse)\n",
    "        print(f\"  ‚Üí Building parcels (landuse)...\")\n",
    "        try:\n",
    "            parcels = ox.features_from_point(\n",
    "                (lat, lon),\n",
    "                dist=RADIUS,\n",
    "                tags={'landuse': True}\n",
    "            )\n",
    "            parcels_proj = parcels.to_crs(ox.graph_to_gdfs(G_proj, nodes=False).crs)\n",
    "            parcels_proj = parcels_proj[parcels_proj.geometry.type.isin(['Polygon', 'MultiPolygon'])].copy()\n",
    "            parcels_proj['geometry'] = parcels_proj.geometry.apply(get_polygon)\n",
    "            parcels_proj = parcels_proj[parcels_proj.geometry.type == 'Polygon'].copy()\n",
    "            print(f\"    ‚úì Found {len(parcels_proj)} parcels\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö† No parcels found: {e}\")\n",
    "            parcels_proj = gpd.GeoDataFrame(columns=['geometry', 'landuse'], crs=ox.graph_to_gdfs(G_proj, nodes=False).crs)\n",
    "        \n",
    "        # Store data\n",
    "        city_data[city_key] = {\n",
    "            'name': city_info['name'],\n",
    "            'color': city_info['color'],\n",
    "            'coords': (lat, lon),\n",
    "            'graph': G_proj,\n",
    "            'buildings': buildings_proj,\n",
    "            'parcels': parcels_proj,\n",
    "            'crs': ox.graph_to_gdfs(G_proj, nodes=False).crs\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úì Downloaded:\")\n",
    "        print(f\"    - {G_proj.number_of_nodes()} nodes\")\n",
    "        print(f\"    - {G_proj.number_of_edges()} edges\")\n",
    "        print(f\"    - {len(buildings_proj)} buildings\")\n",
    "        print(f\"    - {len(parcels_proj)} parcels\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Data acquisition complete for {len(city_data)} cities\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 NEW: Base Map Visualization\n",
    "Display the full urban context before analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base maps showing roads + buildings (Option B: Vector data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Creating base maps...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), facecolor='white')\n",
    "\n",
    "for idx, city_key in enumerate(city_data.keys()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    _, edges = ox.graph_to_gdfs(city_data[city_key]['graph'])\n",
    "    buildings = city_data[city_key]['buildings']\n",
    "    parcels = city_data[city_key]['parcels']\n",
    "    \n",
    "    # Plot parcels in light green (if available)\n",
    "    if len(parcels) > 0:\n",
    "        parcels.plot(ax=ax, color='#E8F5E9', edgecolor='#66BB6A', linewidth=0.5, alpha=0.3)\n",
    "    \n",
    "    # Plot buildings in gray\n",
    "    buildings.plot(ax=ax, color='#BDBDBD', edgecolor='#424242', linewidth=0.3, alpha=0.8)\n",
    "    \n",
    "    # Plot roads in black\n",
    "    edges.plot(ax=ax, color='#000000', linewidth=1.5, alpha=0.9)\n",
    "    \n",
    "    ax.set_title(\n",
    "        f\"{city_data[city_key]['name']}\\n\"\n",
    "        f\"{len(buildings)} buildings ¬∑ {len(edges)} roads ¬∑ {len(parcels)} parcels\",\n",
    "        fontsize=14, fontweight='bold', pad=15\n",
    "    )\n",
    "    ax.set_xlabel('Easting (m)', fontsize=11)\n",
    "    ax.set_ylabel('Northing (m)', fontsize=11)\n",
    "    ax.tick_params(labelsize=9)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.suptitle('Base Maps: Urban Context (500√ó500m)', fontsize=22, fontweight='bold', y=1.0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save both formats\n",
    "plt.savefig(VIZ_PNG_DIR / '00_base_maps.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.savefig(VIZ_SVG_DIR / '00_base_maps.svg', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: 00_base_maps.png + 00_base_maps.svg\")\n",
    "print(\"  (Buildings in gray, roads in black, parcels in light green)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Node Analysis (Centrality Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_node_metrics(G):\n",
    "    \"\"\"Compute centrality metrics for nodes\"\"\"\n",
    "    print(\"  Computing node centrality...\")\n",
    "    \n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    print(\"    - Betweenness (distance)...\")\n",
    "    bc_dist = nx.betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    bc_info = nx.betweenness_centrality(G_undir, weight=None, normalized=True)\n",
    "    \n",
    "    # Closeness centrality\n",
    "    print(\"    - Closeness...\")\n",
    "    closeness = nx.closeness_centrality(G_undir, distance='length')\n",
    "    \n",
    "    # Reach centrality\n",
    "    print(\"    - Reach (200m, 300m)...\")\n",
    "    reach_200 = {}\n",
    "    reach_300 = {}\n",
    "    for node in G_undir.nodes():\n",
    "        reach_200[node] = len(nx.single_source_dijkstra_path_length(G_undir, node, cutoff=200, weight='length'))\n",
    "        reach_300[node] = len(nx.single_source_dijkstra_path_length(G_undir, node, cutoff=300, weight='length'))\n",
    "    \n",
    "    degree = dict(G_undir.degree())\n",
    "    \n",
    "    nodes, _ = ox.graph_to_gdfs(G)\n",
    "    nodes['bc_distance'] = nodes.index.map(bc_dist)\n",
    "    nodes['bc_information'] = nodes.index.map(bc_info)\n",
    "    nodes['closeness'] = nodes.index.map(closeness)\n",
    "    nodes['reach_200m'] = nodes.index.map(reach_200)\n",
    "    nodes['reach_300m'] = nodes.index.map(reach_300)\n",
    "    nodes['degree'] = nodes.index.map(degree)\n",
    "    \n",
    "    print(\"  ‚úì Node metrics computed\")\n",
    "    return nodes\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    city_data[city_key]['nodes'] = compute_node_metrics(city_data[city_key]['graph'])\n",
    "    city_data[city_key]['nodes'].to_file(GEOJSON_DIR / f\"{city_key}_nodes.geojson\", driver='GeoJSON')\n",
    "    print(f\"  ‚úì Saved to {city_key}_nodes.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Edge Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_metrics(G):\n",
    "    \"\"\"Compute edge metrics\"\"\"\n",
    "    print(\"  Computing edge metrics...\")\n",
    "    \n",
    "    G_undir = G.to_undirected()\n",
    "    \n",
    "    print(\"    - Edge betweenness...\")\n",
    "    edge_bc = nx.edge_betweenness_centrality(G_undir, weight='length', normalized=True)\n",
    "    \n",
    "    # Dual graph for angular analysis\n",
    "    print(\"    - Angular betweenness...\")\n",
    "    dual_G = nx.Graph()\n",
    "    edge_to_node = {}\n",
    "    for i, (u, v, k) in enumerate(G_undir.edges(keys=True)):\n",
    "        edge_to_node[(u, v, k)] = i\n",
    "        dual_G.add_node(i, primal_edge=(u, v, k))\n",
    "    \n",
    "    for node in G_undir.nodes():\n",
    "        incident_edges = list(G_undir.edges(node, keys=True))\n",
    "        for i in range(len(incident_edges)):\n",
    "            for j in range(i+1, len(incident_edges)):\n",
    "                e1, e2 = incident_edges[i], incident_edges[j]\n",
    "                e1_norm = tuple(sorted([e1[0], e1[1]])) + (e1[2],)\n",
    "                e2_norm = tuple(sorted([e2[0], e2[1]])) + (e2[2],)\n",
    "                if e1_norm in edge_to_node and e2_norm in edge_to_node:\n",
    "                    dual_G.add_edge(edge_to_node[e1_norm], edge_to_node[e2_norm])\n",
    "    \n",
    "    dual_bc = nx.betweenness_centrality(dual_G, weight=None, normalized=True) if dual_G.number_of_edges() > 0 else {}\n",
    "    angular_bc = {}\n",
    "    for dual_node, bc_val in dual_bc.items():\n",
    "        primal_edge = dual_G.nodes[dual_node].get('primal_edge')\n",
    "        if primal_edge:\n",
    "            angular_bc[primal_edge] = bc_val\n",
    "    \n",
    "    _, edges = ox.graph_to_gdfs(G)\n",
    "    edges['edge_bc'] = edges.index.map(lambda x: edge_bc.get((x[0], x[1]), 0))\n",
    "    edges['angular_bc'] = edges.index.map(lambda x: angular_bc.get(x, 0))\n",
    "    \n",
    "    print(\"  ‚úì Edge metrics computed\")\n",
    "    return edges\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    city_data[city_key]['edges'] = compute_edge_metrics(city_data[city_key]['graph'])\n",
    "    city_data[city_key]['edges'].to_file(GEOJSON_DIR / f\"{city_key}_edges.geojson\", driver='GeoJSON')\n",
    "    print(f\"  ‚úì Saved to {city_key}_edges.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parcel Analysis\n",
    "Process building parcels (landuse boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process parcels and compute metrics\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    parcels = city_data[city_key]['parcels']\n",
    "    \n",
    "    if len(parcels) > 0:\n",
    "        print(\"  Processing parcels...\")\n",
    "        parcels['area'] = parcels.geometry.area\n",
    "        parcels['perimeter'] = parcels.geometry.length\n",
    "        parcels['compactness'] = (4 * np.pi * parcels['area']) / (parcels['perimeter'] ** 2)\n",
    "        \n",
    "        # Filter by size\n",
    "        parcels_filtered = parcels[\n",
    "            (parcels['area'] >= MIN_PARCEL_AREA) & \n",
    "            (parcels['area'] <= MAX_PARCEL_AREA)\n",
    "        ].copy()\n",
    "        \n",
    "        # Compute aspect ratio\n",
    "        aspect_ratios = []\n",
    "        for geom in parcels_filtered.geometry:\n",
    "            try:\n",
    "                mbr = geom.minimum_rotated_rectangle\n",
    "                coords = list(mbr.exterior.coords)\n",
    "                side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "                side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "                aspect = max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0\n",
    "                aspect_ratios.append(aspect)\n",
    "            except:\n",
    "                aspect_ratios.append(1.0)\n",
    "        \n",
    "        parcels_filtered['aspect_ratio'] = aspect_ratios\n",
    "        parcels_filtered['parcel_id'] = [f\"parcel_{i:03d}\" for i in range(len(parcels_filtered))]\n",
    "        \n",
    "        city_data[city_key]['parcels_processed'] = parcels_filtered\n",
    "        \n",
    "        # Save\n",
    "        parcels_filtered.to_file(GEOJSON_DIR / f\"{city_key}_parcels.geojson\", driver='GeoJSON')\n",
    "        print(f\"  ‚úì Processed {len(parcels_filtered)} parcels (filtered from {len(parcels)})\")\n",
    "        print(f\"  ‚úì Saved to {city_key}_parcels.geojson\")\n",
    "    else:\n",
    "        print(\"  ‚ö† No parcels available\")\n",
    "        city_data[city_key]['parcels_processed'] = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. District Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# NetworkX has built-in Louvain community detection (since v2.5+)\n# No external packages needed\nprint(\"‚úì Using NetworkX built-in Louvain community detection\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def detect_districts(G, method='distance'):\n    \"\"\"Detect districts using community detection\"\"\"\n    print(f\"    - {method}...\")\n    try:\n        G_undir = G.to_undirected()\n        G_simple = nx.Graph()\n        for u, v, data in G_undir.edges(data=True):\n            if not G_simple.has_edge(u, v):\n                G_simple.add_edge(u, v, **data)\n        \n        # Use NetworkX built-in Louvain\n        if method == 'distance':\n            communities = nx.algorithms.community.louvain_communities(G_simple, weight='length')\n        else:\n            communities = nx.algorithms.community.louvain_communities(G_simple, weight=None)\n        \n        # Convert from list of sets to node->community_id dict\n        partition = {}\n        for comm_id, community in enumerate(communities):\n            for node in community:\n                partition[node] = comm_id\n        \n        return partition\n    except Exception as e:\n        print(f\"      ‚úó Error: {e}\")\n        return {node: 0 for node in G.nodes()}\n\nfor city_key in city_data.keys():\n    print(f\"\\n{city_data[city_key]['name']}:\")\n    G = city_data[city_key]['graph']\n    nodes = city_data[city_key]['nodes']\n    \n    partitions = {}\n    for method in ['distance', 'angular', 'topological']:\n        partition = detect_districts(G, method=method)\n        partitions[method] = partition\n        \n        nodes_districts = nodes.copy()\n        nodes_districts['district'] = nodes_districts.index.map(partition)\n        nodes_districts.to_file(GEOJSON_DIR / f\"{city_key}_districts_{method}.geojson\", driver='GeoJSON')\n        \n        print(f\"      {method}: {len(set(partition.values()))} districts\")\n    \n    city_data[city_key]['partitions'] = partitions\n    print(f\"  ‚úì District detection complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Landmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_normalize(series, default=0.5):\n",
    "    \"\"\"Safely normalize, handling NaN and min==max\"\"\"\n",
    "    min_val, max_val = series.min(), series.max()\n",
    "    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:\n",
    "        return pd.Series([default] * len(series), index=series.index)\n",
    "    return ((series - min_val) / (max_val - min_val)).fillna(default)\n",
    "\n",
    "def compute_landmark_scores(buildings_gdf, edges_gdf):\n",
    "    \"\"\"Compute landmark scores\"\"\"\n",
    "    print(\"  Computing landmark scores...\")\n",
    "    buildings = buildings_gdf.copy()\n",
    "    \n",
    "    # Structural score\n",
    "    buildings['area'] = buildings.geometry.area\n",
    "    buildings['s_area'] = safe_normalize(buildings['area'])\n",
    "    \n",
    "    street_union = unary_union(edges_gdf.geometry)\n",
    "    buildings['dist_to_street'] = buildings.geometry.apply(lambda g: g.distance(street_union))\n",
    "    max_dist = buildings['dist_to_street'].max()\n",
    "    buildings['s_visibility'] = (1 - buildings['dist_to_street'] / max_dist) if max_dist > 0 else 0.5\n",
    "    buildings['s_visibility'] = buildings['s_visibility'].fillna(0.5)\n",
    "    buildings['structural_score'] = (0.6 * buildings['s_area'] + 0.4 * buildings['s_visibility']).fillna(0.5)\n",
    "    \n",
    "    # Other scores\n",
    "    buildings['visual_score'] = 0.5\n",
    "    buildings['cultural_score'] = 0.0\n",
    "    buildings['pragmatic_score'] = 0.0\n",
    "    buildings['global_score'] = (0.4 * buildings['structural_score'] + 0.2 * buildings['visual_score'] + \n",
    "                                   0.2 * buildings['cultural_score'] + 0.2 * buildings['pragmatic_score']).fillna(0.5)\n",
    "    \n",
    "    # Geometry metrics\n",
    "    aspect_ratios = []\n",
    "    for geom in buildings.geometry:\n",
    "        try:\n",
    "            mbr = geom.minimum_rotated_rectangle\n",
    "            coords = list(mbr.exterior.coords)\n",
    "            side1 = Point(coords[0]).distance(Point(coords[1]))\n",
    "            side2 = Point(coords[1]).distance(Point(coords[2]))\n",
    "            aspect_ratios.append(max(side1, side2) / min(side1, side2) if min(side1, side2) > 0 else 1.0)\n",
    "        except:\n",
    "            aspect_ratios.append(1.0)\n",
    "    buildings['aspect_ratio'] = aspect_ratios\n",
    "    buildings['setback_dist'] = buildings['dist_to_street']\n",
    "    \n",
    "    print(\"  ‚úì Landmark scores computed\")\n",
    "    return buildings\n",
    "\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    buildings_scored = compute_landmark_scores(city_data[city_key]['buildings'], city_data[city_key]['edges'])\n",
    "    city_data[city_key]['buildings_scored'] = buildings_scored\n",
    "    \n",
    "    cols = ['geometry', 'area', 'structural_score', 'global_score', 'aspect_ratio', 'setback_dist']\n",
    "    buildings_scored[cols].to_file(GEOJSON_DIR / f\"{city_key}_buildings.geojson\", driver='GeoJSON')\n",
    "    print(f\"  ‚úì Saved to {city_key}_buildings.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Building Footprint Library\n",
    "Extract diverse individual building footprints (not blocks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_building_footprint_library(buildings_gdf, city_key, target_count=35):\n",
    "    \"\"\"Extract diverse building footprints\"\"\"\n",
    "    print(f\"  Extracting {target_count} footprints...\")\n",
    "    \n",
    "    if len(buildings_gdf) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by area for diversity\n",
    "    buildings = buildings_gdf.copy().sort_values('area')\n",
    "    \n",
    "    if len(buildings) <= target_count:\n",
    "        selected = buildings\n",
    "    else:\n",
    "        indices = np.linspace(0, len(buildings)-1, target_count, dtype=int)\n",
    "        selected = buildings.iloc[indices]\n",
    "    \n",
    "    library = []\n",
    "    for idx, (_, row) in enumerate(selected.iterrows()):\n",
    "        geom = row.geometry\n",
    "        centroid = geom.centroid\n",
    "        \n",
    "        # Translate to origin\n",
    "        translated = translate(geom, xoff=-centroid.x, yoff=-centroid.y)\n",
    "        \n",
    "        library.append({\n",
    "            'footprint_id': f\"{city_key}_building_{idx:03d}\",\n",
    "            'city': city_key,\n",
    "            'area': float(row['area']),\n",
    "            'aspect_ratio': float(row.get('aspect_ratio', 1.0)),\n",
    "            'structural_score': float(row.get('structural_score', 0.5)),\n",
    "            'geometry': {\n",
    "                'type': 'Polygon',\n",
    "                'coordinates': [list(translated.exterior.coords)]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    print(f\"  ‚úì Extracted {len(library)} footprints\")\n",
    "    return library\n",
    "\n",
    "# Extract for all cities\n",
    "all_footprints = []\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\n{city_data[city_key]['name']}:\")\n",
    "    footprints = extract_building_footprint_library(\n",
    "        city_data[city_key]['buildings_scored'],\n",
    "        city_key,\n",
    "        FOOTPRINTS_PER_CITY\n",
    "    )\n",
    "    all_footprints.extend(footprints)\n",
    "    city_data[city_key]['footprint_library'] = footprints\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Total footprint library: {len(all_footprints)} buildings\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save\n",
    "if len(all_footprints) > 0:\n",
    "    with open(METRICS_DIR / 'building_footprint_library.json', 'w') as f:\n",
    "        json.dump(all_footprints, f, indent=2)\n",
    "    print(f\"‚úì Saved to building_footprint_library.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution(values, bins=20):\n",
    "    if len(values) == 0:\n",
    "        return {'bins': [], 'counts': [], 'mean': 0, 'median': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "    hist, bin_edges = np.histogram(values, bins=bins)\n",
    "    return {\n",
    "        'bins': bin_edges.tolist(),\n",
    "        'counts': hist.tolist(),\n",
    "        'mean': float(np.mean(values)),\n",
    "        'median': float(np.median(values)),\n",
    "        'std': float(np.std(values)),\n",
    "        'min': float(np.min(values)),\n",
    "        'max': float(np.max(values))\n",
    "    }\n",
    "\n",
    "urban_metrics = {}\n",
    "for city_key in city_data.keys():\n",
    "    print(f\"\\nAggregating: {city_data[city_key]['name']}...\")\n",
    "    \n",
    "    nodes = city_data[city_key]['nodes']\n",
    "    edges = city_data[city_key]['edges']\n",
    "    parcels = city_data[city_key]['parcels_processed']\n",
    "    buildings = city_data[city_key]['buildings_scored']\n",
    "    partitions = city_data[city_key]['partitions']\n",
    "    \n",
    "    urban_metrics[city_key] = {\n",
    "        'name': city_data[city_key]['name'],\n",
    "        'nodes': {\n",
    "            'total_count': len(nodes),\n",
    "            'avg_degree': float(nodes['degree'].mean()),\n",
    "            'degree_distribution': nodes['degree'].value_counts().to_dict()\n",
    "        },\n",
    "        'edges': {\n",
    "            'total_count': len(edges),\n",
    "            'total_length_km': float(edges['length'].sum() / 1000),\n",
    "            'density_km_per_km2': float((edges['length'].sum() / 1000) / 0.25),\n",
    "            'segment_length_distribution': compute_distribution(edges['length'].values)\n",
    "        },\n",
    "        'parcels': {\n",
    "            'total_count': len(parcels),\n",
    "            'area_distribution': compute_distribution(parcels['area'].values) if len(parcels) > 0 else {}\n",
    "        },\n",
    "        'buildings': {\n",
    "            'total_count': len(buildings),\n",
    "            'area_distribution': compute_distribution(buildings['area'].values),\n",
    "            'aspect_ratio_distribution': compute_distribution(buildings['aspect_ratio'].values)\n",
    "        },\n",
    "        'districts': {\n",
    "            'count_distance': len(set(partitions['distance'].values())),\n",
    "            'count_angular': len(set(partitions['angular'].values())),\n",
    "            'count_topological': len(set(partitions['topological'].values()))\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(METRICS_DIR / 'urban_metrics.json', 'w') as f:\n",
    "    json.dump({'urban_metrics': urban_metrics}, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì Saved to urban_metrics.json\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Metrics Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä METRICS SUMMARY TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "table_data = []\n",
    "metrics = [\n",
    "    ('Nodes', lambda m: m['nodes']['total_count']),\n",
    "    ('Edges', lambda m: m['edges']['total_count']),\n",
    "    ('Street Length (km)', lambda m: m['edges']['total_length_km']),\n",
    "    ('Street Density (km/km¬≤)', lambda m: m['edges']['density_km_per_km2']),\n",
    "    ('Avg Segment (m)', lambda m: m['edges']['segment_length_distribution']['mean']),\n",
    "    ('Parcels', lambda m: m['parcels']['total_count']),\n",
    "    ('Buildings', lambda m: m['buildings']['total_count']),\n",
    "    ('Avg Building Area (m¬≤)', lambda m: m['buildings']['area_distribution']['mean']),\n",
    "    ('Districts (distance)', lambda m: m['districts']['count_distance'])\n",
    "]\n",
    "\n",
    "for metric_name, func in metrics:\n",
    "    row = {'Metric': metric_name}\n",
    "    for city_key in city_data.keys():\n",
    "        try:\n",
    "            row[city_data[city_key]['name']] = f\"{func(urban_metrics[city_key]):.1f}\"\n",
    "        except:\n",
    "            row[city_data[city_key]['name']] = 'N/A'\n",
    "    table_data.append(row)\n",
    "\n",
    "df = pd.DataFrame(table_data).set_index('Metric')\n",
    "print(\"\\n\" + df.to_string())\n",
    "\n",
    "df.to_csv(METRICS_DIR / 'metrics_summary.csv')\n",
    "print(f\"\\n‚úì Saved to metrics_summary.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizations (All SVG + PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Betweenness comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8), facecolor='#1a1a1a')\n",
    "\n",
    "for idx, city_key in enumerate(city_data.keys()):\n",
    "    ax = axes[idx]\n",
    "    edges = city_data[city_key]['edges']\n",
    "    edges.plot(ax=ax, column='angular_bc', cmap='YlOrRd', linewidth=2, legend=False)\n",
    "    ax.set_title(city_data[city_key]['name'], fontsize=20, color='white')\n",
    "    ax.axis('off')\n",
    "    ax.set_facecolor('#1a1a1a')\n",
    "\n",
    "plt.suptitle('Angular Betweenness: Urban Movement', fontsize=24, color='white')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_PNG_DIR / 'betweenness_comparison.png', dpi=300, facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'betweenness_comparison.svg', facecolor='#1a1a1a', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úì Saved: betweenness_comparison (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Footprint library visualization\n",
    "if len(all_footprints) > 0:\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(20, 15), facecolor='white')\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    selected = []\n",
    "    for city_key in city_data.keys():\n",
    "        lib = city_data[city_key]['footprint_library']\n",
    "        if len(lib) >= 4:\n",
    "            selected.extend([lib[0], lib[len(lib)//3], lib[2*len(lib)//3], lib[-1]])\n",
    "    \n",
    "    for idx, fp in enumerate(selected[:12]):\n",
    "        ax = axes[idx]\n",
    "        poly = Polygon(fp['geometry']['coordinates'][0])\n",
    "        x, y = poly.exterior.xy\n",
    "        ax.fill(x, y, color='black')\n",
    "        ax.set_title(f\"{fp['city'].upper()}\\n{fp['area']:.0f} m¬≤ | AR: {fp['aspect_ratio']:.1f}\", fontsize=10)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for idx in range(len(selected), 12):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('Building Footprint Library', fontsize=24)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(VIZ_PNG_DIR / 'footprint_library.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(VIZ_SVG_DIR / 'footprint_library.svg', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"‚úì Saved: footprint_library (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative histograms\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14), facecolor='white')\n",
    "\n",
    "distributions = [\n",
    "    ('edges', 'segment_length_distribution', 'Street Segment Length', 'Length (m)'),\n",
    "    ('parcels', 'area_distribution', 'Parcel Area', 'Area (m¬≤)'),\n",
    "    ('buildings', 'area_distribution', 'Building Area', 'Area (m¬≤)'),\n",
    "    ('buildings', 'aspect_ratio_distribution', 'Building Aspect Ratio', 'Ratio')\n",
    "]\n",
    "\n",
    "for idx, (cat, key, title, xlabel) in enumerate(distributions):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    for city_key in city_data.keys():\n",
    "        metric = urban_metrics[city_key][cat].get(key, {})\n",
    "        if 'bins' in metric and len(metric['bins']) > 1:\n",
    "            centers = [(metric['bins'][i] + metric['bins'][i+1])/2 for i in range(len(metric['bins'])-1)]\n",
    "            ax.plot(centers, metric['counts'], \n",
    "                   label=f\"{city_data[city_key]['name']} (Œº={metric['mean']:.1f})\",\n",
    "                   color=city_data[city_key]['color'], linewidth=2.5, alpha=0.8)\n",
    "    ax.set_xlabel(xlabel, fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=15, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Comparative Distributions', fontsize=20, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_PNG_DIR / 'comparative_histograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(VIZ_SVG_DIR / 'comparative_histograms.svg', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"‚úì Saved: comparative_histograms (PNG + SVG)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì STEP 1 COMPLETE: URBAN ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìÅ OUTPUTS:\")\n",
    "print(f\"  GeoJSON: {len(list(GEOJSON_DIR.glob('*.geojson')))} files\")\n",
    "print(f\"  PNG: {len(list(VIZ_PNG_DIR.glob('*.png')))} files\")\n",
    "print(f\"  SVG: {len(list(VIZ_SVG_DIR.glob('*.svg')))} files\")\n",
    "print(f\"  Metrics: {len(list(METRICS_DIR.glob('*')))} files\")\n",
    "\n",
    "print(\"\\nüìä RESULTS:\")\n",
    "for city_key in city_data.keys():\n",
    "    m = urban_metrics[city_key]\n",
    "    print(f\"  {m['name']}: {m['buildings']['total_count']} buildings, \"\n",
    "          f\"{m['parcels']['total_count']} parcels, \"\n",
    "          f\"{len(city_data[city_key]['footprint_library'])} in library\")\n",
    "\n",
    "print(f\"\\n  TOTAL FOOTPRINT LIBRARY: {len(all_footprints)} buildings\")\n",
    "\n",
    "print(\"\\n‚úì All visualizations: PNG + SVG\")\n",
    "print(\"‚úì Building footprints: Individual shapes (not blocks)\")\n",
    "print(\"‚úì Parcels: Landuse boundaries from OSM\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}